{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1758839791622,
     "user": {
      "displayName": "Oscar Zhang",
      "userId": "08884929965206842440"
     },
     "user_tz": -60
    },
    "id": "fWGYfkhF4JUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? True\n",
      "Device: NVIDIA A2\n",
      "Using device: cuda\n",
      "\n",
      "=== Sweep N_f = 500 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 2.718e+01 | Val MSE: 2.418e-01 | Max Val |err|: 1.140e+00 | Time: 0.17s\n",
      "Epochs:   1000 | TrainLoss: 1.895e-01 | Val MSE: 3.591e-04 | Max Val |err|: 7.773e-02 | Time: 3.41s\n",
      "Epochs:   2000 | TrainLoss: 2.222e-02 | Val MSE: 4.908e-05 | Max Val |err|: 2.909e-02 | Time: 3.30s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=6.504e-03, Val MSE=1.181e-05)\n",
      "Epochs:   3000 | TrainLoss: 6.500e-03 | Val MSE: 8.097e-06 | Max Val |err|: 9.649e-03 | Time: 3.29s\n",
      "Epochs:   4000 | TrainLoss: 1.043e-05 | Val MSE: 1.824e-08 | Max Val |err|: 5.588e-04 | Time: 33.12s\n",
      "Epochs:   5000 | TrainLoss: 1.043e-05 | Val MSE: 1.824e-08 | Max Val |err|: 5.588e-04 | Time: 8.35s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 59.86 s\n",
      "Best Val MSE: 1.824e-08 at epoch 3171\n",
      "Best Max |err| on validation: 5.588e-04\n",
      "\n",
      "=== Sweep N_f = 1000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 3.558e+01 | Val MSE: 2.014e-01 | Max Val |err|: 9.794e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 2.404e-01 | Val MSE: 3.461e-04 | Max Val |err|: 6.148e-02 | Time: 3.38s\n",
      "Epochs:   2000 | TrainLoss: 3.857e-02 | Val MSE: 5.531e-05 | Max Val |err|: 2.945e-02 | Time: 3.32s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2997 (TrainLoss=1.001e-02, Val MSE=2.406e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.209e-02 | Val MSE: 1.259e-05 | Max Val |err|: 1.636e-02 | Time: 3.31s\n",
      "Epochs:   4000 | TrainLoss: 1.253e-05 | Val MSE: 3.279e-08 | Max Val |err|: 8.901e-04 | Time: 30.52s\n",
      "Epochs:   5000 | TrainLoss: 1.253e-05 | Val MSE: 3.279e-08 | Max Val |err|: 8.901e-04 | Time: 8.29s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 57.20 s\n",
      "Best Val MSE: 3.279e-08 at epoch 3150\n",
      "Best Max |err| on validation: 8.901e-04\n",
      "\n",
      "=== Sweep N_f = 2000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 4.420e+01 | Val MSE: 1.814e-01 | Max Val |err|: 1.113e+00 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 3.014e-01 | Val MSE: 4.800e-04 | Max Val |err|: 9.421e-02 | Time: 3.47s\n",
      "Epochs:   2000 | TrainLoss: 4.579e-02 | Val MSE: 5.889e-05 | Max Val |err|: 3.496e-02 | Time: 3.48s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2991 (TrainLoss=1.387e-02, Val MSE=1.495e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.388e-02 | Val MSE: 8.379e-06 | Max Val |err|: 1.302e-02 | Time: 3.51s\n",
      "Epochs:   4000 | TrainLoss: 1.061e-05 | Val MSE: 1.541e-08 | Max Val |err|: 5.040e-04 | Time: 31.86s\n",
      "Epochs:   5000 | TrainLoss: 1.061e-05 | Val MSE: 1.541e-08 | Max Val |err|: 5.040e-04 | Time: 8.39s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 59.15 s\n",
      "Best Val MSE: 1.541e-08 at epoch 3162\n",
      "Best Max |err| on validation: 5.040e-04\n",
      "\n",
      "=== Sweep N_f = 4000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 1.983e+01 | Val MSE: 1.310e-01 | Max Val |err|: 6.705e-01 | Time: 0.01s\n",
      "Epochs:   1000 | TrainLoss: 1.876e-01 | Val MSE: 3.189e-04 | Max Val |err|: 9.900e-02 | Time: 4.32s\n",
      "Epochs:   2000 | TrainLoss: 5.327e-02 | Val MSE: 6.809e-05 | Max Val |err|: 4.770e-02 | Time: 4.22s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.839e-02, Val MSE=1.716e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.838e-02 | Val MSE: 1.981e-05 | Max Val |err|: 1.738e-02 | Time: 4.25s\n",
      "Epochs:   4000 | TrainLoss: 1.124e-05 | Val MSE: 1.426e-08 | Max Val |err|: 4.252e-04 | Time: 47.94s\n",
      "Epochs:   5000 | TrainLoss: 1.124e-05 | Val MSE: 1.426e-08 | Max Val |err|: 4.252e-04 | Time: 9.28s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 79.21 s\n",
      "Best Val MSE: 1.426e-08 at epoch 3237\n",
      "Best Max |err| on validation: 4.252e-04\n",
      "\n",
      "N_f sweep summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_i</th>\n",
       "      <th>N_b</th>\n",
       "      <th>N_k</th>\n",
       "      <th>N_f</th>\n",
       "      <th>total_elapsed</th>\n",
       "      <th>best_ep</th>\n",
       "      <th>best_TL</th>\n",
       "      <th>best_v</th>\n",
       "      <th>best_max_err</th>\n",
       "      <th>rel_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>500</td>\n",
       "      <td>60.846287</td>\n",
       "      <td>3171</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.823563e-08</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>57.199393</td>\n",
       "      <td>3150</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>3.279223e-08</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>2000</td>\n",
       "      <td>59.147308</td>\n",
       "      <td>3162</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.541387e-08</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>4000</td>\n",
       "      <td>79.209019</td>\n",
       "      <td>3237</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.426006e-08</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N_i  N_b  N_k   N_f  total_elapsed  best_ep   best_TL        best_v  \\\n",
       "0  101   51   51   500      60.846287     3171  0.000010  1.823563e-08   \n",
       "1  101   51   51  1000      57.199393     3150  0.000013  3.279223e-08   \n",
       "2  101   51   51  2000      59.147308     3162  0.000011  1.541387e-08   \n",
       "3  101   51   51  4000      79.209019     3237  0.000011  1.426006e-08   \n",
       "\n",
       "   best_max_err    rel_L2  \n",
       "0      0.000559  0.000405  \n",
       "1      0.000890  0.000384  \n",
       "2      0.000504  0.000233  \n",
       "3      0.000425  0.000289  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sweep N_i = 21 (N_b=51, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 1.227e+01 | Val MSE: 1.258e-01 | Max Val |err|: 9.034e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 7.193e-02 | Val MSE: 1.503e-04 | Max Val |err|: 4.865e-02 | Time: 3.24s\n",
      "Epochs:   2000 | TrainLoss: 1.541e-02 | Val MSE: 4.418e-05 | Max Val |err|: 2.225e-02 | Time: 3.19s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=4.601e-03, Val MSE=1.285e-05)\n",
      "Epochs:   3000 | TrainLoss: 4.596e-03 | Val MSE: 1.156e-05 | Max Val |err|: 1.027e-02 | Time: 3.35s\n",
      "Epochs:   4000 | TrainLoss: 9.170e-06 | Val MSE: 2.913e-08 | Max Val |err|: 9.755e-04 | Time: 32.26s\n",
      "Epochs:   5000 | TrainLoss: 9.170e-06 | Val MSE: 2.913e-08 | Max Val |err|: 9.755e-04 | Time: 8.13s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 58.40 s\n",
      "Best Val MSE: 2.913e-08 at epoch 3170\n",
      "Best Max |err| on validation: 9.755e-04\n",
      "\n",
      "=== Sweep N_i = 51 (N_b=51, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 1.059e+01 | Val MSE: 9.804e-02 | Max Val |err|: 7.571e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 6.246e-02 | Val MSE: 1.262e-04 | Max Val |err|: 3.239e-02 | Time: 3.42s\n",
      "Epochs:   2000 | TrainLoss: 1.058e-02 | Val MSE: 2.145e-05 | Max Val |err|: 1.930e-02 | Time: 3.34s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2994 (TrainLoss=4.326e-03, Val MSE=1.224e-05)\n",
      "Epochs:   3000 | TrainLoss: 4.445e-03 | Val MSE: 8.157e-06 | Max Val |err|: 1.194e-02 | Time: 3.23s\n",
      "Epochs:   4000 | TrainLoss: 8.670e-06 | Val MSE: 3.189e-08 | Max Val |err|: 1.188e-03 | Time: 40.93s\n",
      "Epochs:   5000 | TrainLoss: 8.670e-06 | Val MSE: 3.189e-08 | Max Val |err|: 1.188e-03 | Time: 8.19s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 67.39 s\n",
      "Best Val MSE: 3.189e-08 at epoch 3231\n",
      "Best Max |err| on validation: 1.188e-03\n",
      "\n",
      "=== Sweep N_i = 101 (N_b=51, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 7.873e+01 | Val MSE: 2.979e-01 | Max Val |err|: 1.486e+00 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 4.352e-01 | Val MSE: 5.745e-04 | Max Val |err|: 8.985e-02 | Time: 3.35s\n",
      "Epochs:   2000 | TrainLoss: 1.209e-01 | Val MSE: 1.597e-04 | Max Val |err|: 4.772e-02 | Time: 3.36s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=2.968e-02, Val MSE=4.455e-05)\n",
      "Epochs:   3000 | TrainLoss: 2.964e-02 | Val MSE: 4.059e-05 | Max Val |err|: 2.116e-02 | Time: 3.56s\n",
      "Epochs:   4000 | TrainLoss: 9.911e-06 | Val MSE: 4.923e-08 | Max Val |err|: 1.434e-03 | Time: 45.63s\n",
      "Epochs:   5000 | TrainLoss: 9.911e-06 | Val MSE: 4.923e-08 | Max Val |err|: 1.434e-03 | Time: 8.59s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 72.97 s\n",
      "Best Val MSE: 4.923e-08 at epoch 3259\n",
      "Best Max |err| on validation: 1.434e-03\n",
      "\n",
      "=== Sweep N_i = 201 (N_b=51, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 6.150e+01 | Val MSE: 1.786e-01 | Max Val |err|: 1.199e+00 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 3.613e-01 | Val MSE: 6.415e-04 | Max Val |err|: 7.912e-02 | Time: 3.65s\n",
      "Epochs:   2000 | TrainLoss: 7.919e-02 | Val MSE: 1.625e-04 | Max Val |err|: 4.796e-02 | Time: 3.60s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.685e-02, Val MSE=3.483e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.683e-02 | Val MSE: 1.963e-05 | Max Val |err|: 2.212e-02 | Time: 3.61s\n",
      "Epochs:   4000 | TrainLoss: 1.067e-05 | Val MSE: 1.541e-08 | Max Val |err|: 6.299e-04 | Time: 31.80s\n",
      "Epochs:   5000 | TrainLoss: 1.067e-05 | Val MSE: 1.541e-08 | Max Val |err|: 6.299e-04 | Time: 8.55s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 59.84 s\n",
      "Best Val MSE: 1.541e-08 at epoch 3153\n",
      "Best Max |err| on validation: 6.299e-04\n",
      "\n",
      "N_i sweep summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_i</th>\n",
       "      <th>N_b</th>\n",
       "      <th>N_k</th>\n",
       "      <th>N_f</th>\n",
       "      <th>total_elapsed</th>\n",
       "      <th>best_ep</th>\n",
       "      <th>best_TL</th>\n",
       "      <th>best_v</th>\n",
       "      <th>best_max_err</th>\n",
       "      <th>rel_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>58.403573</td>\n",
       "      <td>3170</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2.912629e-08</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.000471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>67.396127</td>\n",
       "      <td>3231</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3.188805e-08</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>72.967144</td>\n",
       "      <td>3259</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>4.922951e-08</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.000327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>59.838214</td>\n",
       "      <td>3153</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.541271e-08</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N_i  N_b  N_k   N_f  total_elapsed  best_ep   best_TL        best_v  \\\n",
       "0   21   51   51  1000      58.403573     3170  0.000009  2.912629e-08   \n",
       "1   51   51   51  1000      67.396127     3231  0.000009  3.188805e-08   \n",
       "2  101   51   51  1000      72.967144     3259  0.000010  4.922951e-08   \n",
       "3  201   51   51  1000      59.838214     3153  0.000011  1.541271e-08   \n",
       "\n",
       "   best_max_err    rel_L2  \n",
       "0      0.000975  0.000471  \n",
       "1      0.001188  0.000384  \n",
       "2      0.001434  0.000327  \n",
       "3      0.000630  0.000291  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sweep N_b = 11 (N_i=101, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 7.947e+01 | Val MSE: 1.697e-01 | Max Val |err|: 1.050e+00 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 4.878e-01 | Val MSE: 9.087e-04 | Max Val |err|: 8.847e-02 | Time: 3.24s\n",
      "Epochs:   2000 | TrainLoss: 1.694e-01 | Val MSE: 3.496e-04 | Max Val |err|: 5.543e-02 | Time: 3.24s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=3.645e-02, Val MSE=8.941e-05)\n",
      "Epochs:   3000 | TrainLoss: 3.640e-02 | Val MSE: 4.524e-05 | Max Val |err|: 2.925e-02 | Time: 3.25s\n",
      "Epochs:   4000 | TrainLoss: 7.938e-06 | Val MSE: 3.369e-08 | Max Val |err|: 8.105e-04 | Time: 35.16s\n",
      "Epochs:   5000 | TrainLoss: 7.938e-06 | Val MSE: 3.369e-08 | Max Val |err|: 8.105e-04 | Time: 8.06s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 61.12 s\n",
      "Best Val MSE: 3.369e-08 at epoch 3193\n",
      "Best Max |err| on validation: 8.105e-04\n",
      "\n",
      "=== Sweep N_b = 21 (N_i=101, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 2.141e+01 | Val MSE: 1.317e-01 | Max Val |err|: 8.937e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 1.463e-01 | Val MSE: 2.905e-04 | Max Val |err|: 5.992e-02 | Time: 3.26s\n",
      "Epochs:   2000 | TrainLoss: 4.963e-02 | Val MSE: 1.319e-04 | Max Val |err|: 3.035e-02 | Time: 3.23s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=6.061e-03, Val MSE=1.378e-05)\n",
      "Epochs:   3000 | TrainLoss: 6.057e-03 | Val MSE: 1.023e-05 | Max Val |err|: 1.532e-02 | Time: 3.27s\n",
      "Epochs:   4000 | TrainLoss: 9.045e-06 | Val MSE: 3.263e-08 | Max Val |err|: 7.696e-04 | Time: 36.45s\n",
      "Epochs:   5000 | TrainLoss: 9.045e-06 | Val MSE: 3.263e-08 | Max Val |err|: 7.696e-04 | Time: 8.19s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 62.75 s\n",
      "Best Val MSE: 3.263e-08 at epoch 3196\n",
      "Best Max |err| on validation: 7.696e-04\n",
      "\n",
      "=== Sweep N_b = 51 (N_i=101, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 1.552e+01 | Val MSE: 1.045e-01 | Max Val |err|: 7.092e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 1.329e-01 | Val MSE: 1.846e-04 | Max Val |err|: 4.681e-02 | Time: 3.34s\n",
      "Epochs:   2000 | TrainLoss: 1.548e-02 | Val MSE: 2.118e-05 | Max Val |err|: 1.854e-02 | Time: 3.33s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2975 (TrainLoss=5.229e-03, Val MSE=8.278e-06)\n",
      "Epochs:   3000 | TrainLoss: 5.229e-03 | Val MSE: 6.258e-06 | Max Val |err|: 8.568e-03 | Time: 3.32s\n",
      "Epochs:   4000 | TrainLoss: 1.233e-05 | Val MSE: 3.959e-08 | Max Val |err|: 1.092e-03 | Time: 32.83s\n",
      "Epochs:   5000 | TrainLoss: 1.233e-05 | Val MSE: 3.959e-08 | Max Val |err|: 1.092e-03 | Time: 8.37s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 59.52 s\n",
      "Best Val MSE: 3.959e-08 at epoch 3164\n",
      "Best Max |err| on validation: 1.092e-03\n",
      "\n",
      "=== Sweep N_b = 101 (N_i=101, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 1.907e+01 | Val MSE: 1.455e-01 | Max Val |err|: 6.345e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 2.128e-01 | Val MSE: 3.392e-04 | Max Val |err|: 6.088e-02 | Time: 4.00s\n",
      "Epochs:   2000 | TrainLoss: 5.633e-02 | Val MSE: 6.392e-05 | Max Val |err|: 2.746e-02 | Time: 3.90s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.915e-02, Val MSE=2.101e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.921e-02 | Val MSE: 1.352e-05 | Max Val |err|: 1.333e-02 | Time: 3.96s\n",
      "Epochs:   4000 | TrainLoss: 7.887e-06 | Val MSE: 2.194e-08 | Max Val |err|: 8.437e-04 | Time: 38.28s\n",
      "Epochs:   5000 | TrainLoss: 7.887e-06 | Val MSE: 2.194e-08 | Max Val |err|: 8.437e-04 | Time: 8.78s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 67.64 s\n",
      "Best Val MSE: 2.194e-08 at epoch 3197\n",
      "Best Max |err| on validation: 8.437e-04\n",
      "\n",
      "N_b sweep summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_i</th>\n",
       "      <th>N_b</th>\n",
       "      <th>N_k</th>\n",
       "      <th>N_f</th>\n",
       "      <th>total_elapsed</th>\n",
       "      <th>best_ep</th>\n",
       "      <th>best_TL</th>\n",
       "      <th>best_v</th>\n",
       "      <th>best_max_err</th>\n",
       "      <th>rel_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>61.125521</td>\n",
       "      <td>3193</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>3.368586e-08</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>21</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>62.755419</td>\n",
       "      <td>3196</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3.263451e-08</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>59.521631</td>\n",
       "      <td>3164</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>3.959374e-08</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.000365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>67.643502</td>\n",
       "      <td>3197</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>2.194299e-08</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N_i  N_b  N_k   N_f  total_elapsed  best_ep   best_TL        best_v  \\\n",
       "0  101   11   51  1000      61.125521     3193  0.000008  3.368586e-08   \n",
       "1  101   21   51  1000      62.755419     3196  0.000009  3.263451e-08   \n",
       "2  101   51   51  1000      59.521631     3164  0.000012  3.959374e-08   \n",
       "3  101  101   51  1000      67.643502     3197  0.000008  2.194299e-08   \n",
       "\n",
       "   best_max_err    rel_L2  \n",
       "0      0.000810  0.000237  \n",
       "1      0.000770  0.000251  \n",
       "2      0.001092  0.000365  \n",
       "3      0.000844  0.000254  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sweep N_k = 11 (N_i=101, N_b=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 1.607e+01 | Val MSE: 9.833e-02 | Max Val |err|: 8.014e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 7.881e-02 | Val MSE: 1.069e-04 | Max Val |err|: 4.050e-02 | Time: 3.34s\n",
      "Epochs:   2000 | TrainLoss: 1.763e-02 | Val MSE: 3.206e-05 | Max Val |err|: 1.829e-02 | Time: 3.13s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=6.518e-03, Val MSE=1.054e-05)\n",
      "Epochs:   3000 | TrainLoss: 6.513e-03 | Val MSE: 9.696e-06 | Max Val |err|: 1.437e-02 | Time: 3.22s\n",
      "Epochs:   4000 | TrainLoss: 9.092e-06 | Val MSE: 1.434e-08 | Max Val |err|: 6.778e-04 | Time: 33.49s\n",
      "Epochs:   5000 | TrainLoss: 9.092e-06 | Val MSE: 1.434e-08 | Max Val |err|: 6.778e-04 | Time: 7.88s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 58.95 s\n",
      "Best Val MSE: 1.434e-08 at epoch 3182\n",
      "Best Max |err| on validation: 6.778e-04\n",
      "\n",
      "=== Sweep N_k = 21 (N_i=101, N_b=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 1.933e+01 | Val MSE: 1.271e-01 | Max Val |err|: 7.613e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 1.628e-01 | Val MSE: 3.084e-04 | Max Val |err|: 5.109e-02 | Time: 3.22s\n",
      "Epochs:   2000 | TrainLoss: 2.974e-02 | Val MSE: 5.582e-05 | Max Val |err|: 2.543e-02 | Time: 3.15s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2987 (TrainLoss=1.091e-02, Val MSE=2.076e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.091e-02 | Val MSE: 1.953e-05 | Max Val |err|: 1.877e-02 | Time: 3.31s\n",
      "Epochs:   4000 | TrainLoss: 7.065e-06 | Val MSE: 1.807e-08 | Max Val |err|: 7.685e-04 | Time: 37.88s\n",
      "Epochs:   5000 | TrainLoss: 7.065e-06 | Val MSE: 1.807e-08 | Max Val |err|: 7.685e-04 | Time: 8.02s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 63.60 s\n",
      "Best Val MSE: 1.807e-08 at epoch 3215\n",
      "Best Max |err| on validation: 7.685e-04\n",
      "\n",
      "=== Sweep N_k = 51 (N_i=101, N_b=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 1.665e+01 | Val MSE: 1.226e-01 | Max Val |err|: 8.251e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 1.785e-01 | Val MSE: 2.569e-04 | Max Val |err|: 4.954e-02 | Time: 3.33s\n",
      "Epochs:   2000 | TrainLoss: 3.985e-02 | Val MSE: 5.106e-05 | Max Val |err|: 2.821e-02 | Time: 3.27s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2997 (TrainLoss=1.357e-02, Val MSE=1.665e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.358e-02 | Val MSE: 1.522e-05 | Max Val |err|: 1.450e-02 | Time: 3.44s\n",
      "Epochs:   4000 | TrainLoss: 8.550e-06 | Val MSE: 1.344e-08 | Max Val |err|: 3.923e-04 | Time: 34.10s\n",
      "Epochs:   5000 | TrainLoss: 8.550e-06 | Val MSE: 1.344e-08 | Max Val |err|: 3.923e-04 | Time: 8.31s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 60.75 s\n",
      "Best Val MSE: 1.344e-08 at epoch 3177\n",
      "Best Max |err| on validation: 3.923e-04\n",
      "\n",
      "=== Sweep N_k = 101 (N_i=101, N_b=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 2.469e+01 | Val MSE: 1.578e-01 | Max Val |err|: 6.503e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 1.772e-01 | Val MSE: 2.853e-04 | Max Val |err|: 5.547e-02 | Time: 3.98s\n",
      "Epochs:   2000 | TrainLoss: 1.357e-02 | Val MSE: 3.810e-05 | Max Val |err|: 1.893e-02 | Time: 3.95s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2987 (TrainLoss=3.894e-03, Val MSE=1.091e-05)\n",
      "Epochs:   3000 | TrainLoss: 3.894e-03 | Val MSE: 8.698e-06 | Max Val |err|: 7.769e-03 | Time: 3.94s\n",
      "Epochs:   4000 | TrainLoss: 7.735e-06 | Val MSE: 1.338e-08 | Max Val |err|: 6.643e-04 | Time: 42.26s\n",
      "Epochs:   5000 | TrainLoss: 7.735e-06 | Val MSE: 1.338e-08 | Max Val |err|: 6.643e-04 | Time: 8.90s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 71.93 s\n",
      "Best Val MSE: 1.338e-08 at epoch 3229\n",
      "Best Max |err| on validation: 6.643e-04\n",
      "\n",
      "N_k sweep summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_i</th>\n",
       "      <th>N_b</th>\n",
       "      <th>N_k</th>\n",
       "      <th>N_f</th>\n",
       "      <th>total_elapsed</th>\n",
       "      <th>best_ep</th>\n",
       "      <th>best_TL</th>\n",
       "      <th>best_v</th>\n",
       "      <th>best_max_err</th>\n",
       "      <th>rel_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>11</td>\n",
       "      <td>1000</td>\n",
       "      <td>58.952453</td>\n",
       "      <td>3182</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1.434039e-08</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>21</td>\n",
       "      <td>1000</td>\n",
       "      <td>63.597830</td>\n",
       "      <td>3215</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.807456e-08</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>60.750520</td>\n",
       "      <td>3177</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1.343627e-08</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>101</td>\n",
       "      <td>1000</td>\n",
       "      <td>71.929724</td>\n",
       "      <td>3229</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.337542e-08</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N_i  N_b  N_k   N_f  total_elapsed  best_ep   best_TL        best_v  \\\n",
       "0  101   51   11  1000      58.952453     3182  0.000009  1.434039e-08   \n",
       "1  101   51   21  1000      63.597830     3215  0.000007  1.807456e-08   \n",
       "2  101   51   51  1000      60.750520     3177  0.000009  1.343627e-08   \n",
       "3  101   51  101  1000      71.929724     3229  0.000008  1.337542e-08   \n",
       "\n",
       "   best_max_err    rel_L2  \n",
       "0      0.000678  0.000210  \n",
       "1      0.000768  0.000239  \n",
       "2      0.000392  0.000248  \n",
       "3      0.000664  0.000272  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== END OF SCRIPT ===\n"
     ]
    }
   ],
   "source": [
    "# For readability: disable warnings from libraries like matplotlib, etc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "# Make sure torch is imported somewhere above this cell:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "from itertools import product, combinations\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as la\n",
    "from pyDOE import lhs\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogLocator, FuncFormatter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# --- Device Setup ---\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Select the most performant device available (CUDA > MPS > CPU)\n",
    "device = (\n",
    "    torch.device('cuda') if torch.cuda.is_available()\n",
    "    else torch.device('mps') if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else torch.device('cpu')\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "def u_true_numpy(X, T, logK):\n",
    "    \"\"\"Vectorised true solution: U = exp(-(pi^2) * T) * sin(pi * X).\"\"\"\n",
    "    K = 10.0**logK    # convert log10(k) → k\n",
    "    return np.exp(- K * (np.pi**2) * T) * np.sin(np.pi * X)\n",
    "\n",
    "def net_u(x, t, logk, model):\n",
    "    \"\"\"\n",
    "    NN input is (x, t, log10(k)).\n",
    "    \"\"\"\n",
    "    X = torch.cat([x, t, logk], dim=1)  # If x and t are each shape (N, 1), then X becomes (N, 2).\n",
    "    u = model(X)\n",
    "    return u\n",
    "\n",
    "# net_f computes the PDE residual\n",
    "# If f ≈ 0 at collocation points, the NN satisfies the equation there\n",
    "def net_f(x, t, logk, model):\n",
    "    x.requires_grad_(True)\n",
    "    t.requires_grad_(True)\n",
    "    # logk.requires_grad_(True)\n",
    "    \n",
    "    u = net_u(x, t, logk, model)\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    \n",
    "    # convert log10(k) -> k = 10^logk\n",
    "    k = 10.0**logk\n",
    "    \n",
    "    f = u_t - k * u_xx\n",
    "    return f\n",
    "\n",
    "class XavierInit(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(XavierInit, self).__init__()\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = torch.sqrt(torch.tensor(2.0 / (in_dim + out_dim)))\n",
    "        self.weight = nn.Parameter(torch.randn(in_dim, out_dim) * xavier_stddev)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "def initialize_NN(layers):\n",
    "    weights = nn.ModuleList()\n",
    "    num_layers = len(layers)\n",
    "    for l in range(num_layers - 1):\n",
    "        layer = XavierInit(size=[layers[l], layers[l + 1]]) # if there was no retutn, how do I get the weight and bias?\n",
    "        weights.append(layer)\n",
    "    return weights\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, layers, lb, ub):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.weights = initialize_NN(layers)\n",
    "        # make lb/ub move with .to(device)\n",
    "        self.register_buffer('lb', torch.as_tensor(lb, dtype=torch.float32))     # <<< CHANGED >>>\n",
    "        self.register_buffer('ub', torch.as_tensor(ub, dtype=torch.float32))     # <<< CHANGED >>>\n",
    "        # self.register_buffer('k', torch.tensor(k_init, dtype=torch.float32))     # <<< CHANGED >>>\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.float()                                                            # <<< CHANGED >>>\n",
    "        lb = self.lb.to(X.device)                                                # <<< CHANGED >>>\n",
    "        ub = self.ub.to(X.device)                                                # <<< CHANGED >>>\n",
    "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        for l in range(len(self.weights) - 1):\n",
    "            H = torch.tanh(self.weights[l](H.float()))     # Is this already a calculation?\n",
    "        Y = self.weights[-1](H)\n",
    "        return Y\n",
    "\n",
    "def train(nEpoch, X, u, X_f, X_val, model, learning_rate):\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # ----- STAGE 1: start with Adam -----\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # when to switch from Adam to L-BFGS\n",
    "    switch_epoch = 3000.0\n",
    "    used_lbfgs   = False   \n",
    "    lbfgs_epochs   = 3000.0           # <-- how many epochs of L-BFGS you want\n",
    "    lbfgs_start_ep = None             # <-- will store the epoch where we switch\n",
    "\n",
    "    # use the model's device\n",
    "    dev = next(model.parameters()).device                                        # <<< CHANGED >>>\n",
    "\n",
    "    x    = X[:, 0:1]\n",
    "    t    = X[:, 1:2]\n",
    "    logk = X[:, 2:3]\n",
    "    # Collocation points (f points)\n",
    "    x_f    = X_f[:, 0:1]\n",
    "    t_f    = X_f[:, 1:2]\n",
    "    logk_f = X_f[:, 2:3]\n",
    "    # Validation points\n",
    "    x_v    = X_val[:, 0:1]\n",
    "    t_v    = X_val[:, 1:2]\n",
    "    logk_v = X_val[:, 2:3]\n",
    "\n",
    "    # True validation solution (analytic)\n",
    "    u_v_true = u_true_numpy(x_v, t_v, logk_v)   # shape (N_val,)\n",
    "\n",
    "    # create tensors ON THE SAME DEVICE\n",
    "    x_tf      = torch.tensor(x,        dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    t_tf      = torch.tensor(t,        dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    logk_tf   = torch.tensor(logk,     dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    u_tf      = torch.tensor(u,        dtype=torch.float32, device=dev)                       # <<< CHANGED >>>\n",
    "    x_f_tf    = torch.tensor(x_f,      dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    t_f_tf    = torch.tensor(t_f,      dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    logk_f_tf = torch.tensor(logk_f,   dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    x_v_tf    = torch.tensor(x_v,      dtype=torch.float32, device=dev)\n",
    "    t_v_tf    = torch.tensor(t_v,      dtype=torch.float32, device=dev)\n",
    "    logk_v_tf = torch.tensor(logk_v,   dtype=torch.float32, device=dev)\n",
    "    u_true_tf = torch.tensor(u_v_true, dtype=torch.float32, device=dev).reshape(-1, 1)\n",
    "\n",
    "    mse_v_hist  = []\n",
    "    loss_values = []\n",
    "    max_errors  = []   # <<< NEW: to store (epoch, max_abs_error) >>>\n",
    "\n",
    "    patience  = 10000          # number of validations without improvement\n",
    "    pat       = 0\n",
    "    best_v    = float('inf')   # best validation MSE\n",
    "    best_TL   = float('inf')   # best training loss corresponding to best_v\n",
    "    best_max_err = float('inf')  # <<< NEW: best max |error| on val\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_ep    = -1\n",
    "\n",
    "    start_time  = time.time()\n",
    "    total_start = time.time()        # total wall-clock timer\n",
    "\n",
    "    for ep in range(nEpoch):\n",
    "\n",
    "        # ----- Stop if we've done lbfgs_epochs of L-BFGS -----\n",
    "        if used_lbfgs and lbfgs_start_ep is not None:\n",
    "            if ep - lbfgs_start_ep >= lbfgs_epochs:\n",
    "                print(f\"Stopping after {lbfgs_epochs} LBFGS epochs at global epoch {ep}\")\n",
    "                break\n",
    "\n",
    "        # ------------------------------\n",
    "        # STAGE 1: Adam (ep < switch_epoch)\n",
    "        # STAGE 2: L-BFGS (ep >= switch_epoch)\n",
    "        # ------------------------------\n",
    "        if ep < switch_epoch:\n",
    "            # ----- Adam update -----\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute predictions for training data (u)\n",
    "            u_pred = net_u(x_tf, t_tf, logk_tf, model)          # <<< CHANGED\n",
    "            # Compute PDE residual at collocation points\n",
    "            u_f_pred = net_f(x_f_tf, t_f_tf, logk_f_tf, model)  # <<< CHANGED\n",
    "    \n",
    "            loss_PDE  = criterion(u_f_pred, torch.zeros_like(u_f_pred))\n",
    "            loss_data = criterion(u_tf, u_pred)\n",
    "            loss = loss_PDE + 100 * loss_data\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        else:\n",
    "            # ----- Switch to L-BFGS once -----\n",
    "            if not used_lbfgs:\n",
    "                # 1) Load the best Adam weights BEFORE creating LBFGS\n",
    "                model.load_state_dict(best_state)\n",
    "        \n",
    "                # 2) Print which Adam state you're starting from\n",
    "                print(\n",
    "                    f\"Switching to L-BFGS at epoch {ep} \"\n",
    "                    f\"-> starting from Adam best at epoch {best_ep} \"\n",
    "                    f\"(TrainLoss={best_TL:.3e}, Val MSE={best_v:.3e})\"\n",
    "                )\n",
    "        \n",
    "                # 3) Create the LBFGS optimiser on top of that state\n",
    "                optimizer = torch.optim.LBFGS(\n",
    "                    model.parameters(),\n",
    "                    max_iter=20,          # internal LBFGS iterations per .step()\n",
    "                    history_size=100,\n",
    "                    line_search_fn=None\n",
    "                )\n",
    "                used_lbfgs = True\n",
    "                lbfgs_start_ep = ep      # <-- remember when we switched\n",
    "                \n",
    "\n",
    "            # L-BFGS requires a closure that re-computes the loss\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                u_pred   = net_u(x_tf, t_tf, logk_tf, model)          # <<< CHANGED\n",
    "                u_f_pred = net_f(x_f_tf, t_f_tf, logk_f_tf, model)    # <<< CHANGED\n",
    "\n",
    "                loss_PDE  = criterion(u_f_pred, torch.zeros_like(u_f_pred))\n",
    "                loss_data = criterion(u_tf, u_pred)\n",
    "                loss      = loss_PDE + 100 * loss_data\n",
    "\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            loss = optimizer.step(closure)  # returns the loss from the last closure call\n",
    "\n",
    "        \n",
    "        # ----- validation -----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            u_v_pred = net_u(x_v_tf, t_v_tf, logk_v_tf, model)   # <<< CHANGED\n",
    "            mse_v = criterion(u_v_pred, u_true_tf).item()\n",
    "            mse_v_hist.append((ep, mse_v))\n",
    "\n",
    "            # <<< NEW: max absolute error on validation set >>>\n",
    "            abs_err = torch.abs(u_v_pred - u_true_tf)   # (N_val, 1)\n",
    "            max_err = abs_err.max().item()              # scalar\n",
    "            max_errors.append((ep, max_err))            # store (epoch, max_err)\n",
    "            # -----------------------------------------------\n",
    "\n",
    "        model.train()  # switch back\n",
    "\n",
    "\n",
    "        # ----- early stopping on val -----\n",
    "        # if mse_v < best_v:\n",
    "        if loss.item() < best_TL:\n",
    "            best_v       = mse_v\n",
    "            best_TL      = loss.item()\n",
    "            best_max_err = max_err          # <<< NEW: store max error at best state\n",
    "            best_state   = copy.deepcopy(model.state_dict())\n",
    "            best_ep      = ep\n",
    "            # print(f\"[Improved] Epoch {ep} | Best Val MSE: {best_v:.3e}\")\n",
    "            pat = 0\n",
    "        else:\n",
    "            pat += 1\n",
    "            if pat >= patience:\n",
    "                print(f\"Early stopping at it={ep}, best Val MSE={best_v:.3e}\")\n",
    "                break\n",
    "        \n",
    "        # Print progress\n",
    "        # - Before LBFGS: every 1000 epochs\n",
    "        # - After LBFGS is enabled: every 100 epochs\n",
    "        if (not used_lbfgs and ep % 1000 == 0) or (used_lbfgs and ep % 1000 == 0):\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Epochs: {ep:6d} | TrainLoss: {loss.item():.3e} \"\n",
    "                  f\"| Val MSE: {mse_v:.3e} \"\n",
    "                  f\"| Max Val |err|: {max_err:.3e} \"   # <<< NEW\n",
    "                  f\"| Time: {elapsed:.2f}s\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "    total_elapsed = time.time() - total_start\n",
    "    print(f\"Total training time: {total_elapsed:.2f} s\")\n",
    "    print(f\"Best Val MSE: {best_v:.3e} at epoch {best_ep}\")\n",
    "    print(f\"Best Max |err| on validation: {best_max_err:.3e}\")   # <<< NEW\n",
    "\n",
    "    model.load_state_dict(best_state)          # <- load best here\n",
    "\n",
    "    return loss_values, mse_v_hist, max_errors, best_ep, best_TL, best_v, best_max_err\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Build dataset for arbitrary (N_i, N_b, N_k, N_f)\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_dataset(N_i, N_b, N_k, N_f, N_val,\n",
    "                  x_min, x_max,\n",
    "                  t_min, t_max,\n",
    "                  k_min, k_max,\n",
    "                  seed):\n",
    "    \"\"\"\n",
    "    Build training and collocation sets for the parametric heat equation.\n",
    "\n",
    "    Returns:\n",
    "        X_u_train : (N_u, 3) array of data points (x, t, log10(k)) for IC + BC.\n",
    "        u_train   : (N_u, 1) IC + BC at X_u_train.\n",
    "        X_f_train : (N_f, 3) collocation points in (x, t, log10(k)).\n",
    "        X_val     : (N_val, 3) validation points in (x, t, log10(k)).\n",
    "        lb, ub    : lower/upper bounds for normalisation in the NN.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- k and logk ---\n",
    "    # We sample k in [k_min, k_max] but represent it via log10(k).\n",
    "    logk_min = np.log10(k_min)\n",
    "    logk_max = np.log10(k_max)\n",
    "    logk_vec = np.linspace(logk_min, logk_max, N_k)  # equally spaced in log10(k)\n",
    "    k_vec    = 10.0**logk_vec                        # corresponding physical k\n",
    "\n",
    "    # --- Initial condition: u(x,0;k) = sin(pi x) ---\n",
    "    # x in [x_min, x_max], t=0, and all logk samples\n",
    "    x_ic = np.linspace(x_min, x_max, N_i)\n",
    "    t_ic = np.array([t_min])   # or [t_min]\n",
    "    x_ic_g, t_ic_g, logk_ic_g = np.meshgrid(x_ic, t_ic, logk_vec, indexing='ij')\n",
    "\n",
    "    x_u_ic    = x_ic_g.ravel()[:, None]\n",
    "    t_u_ic    = t_ic_g.ravel()[:, None]\n",
    "    logk_u_ic = logk_ic_g.ravel()[:, None]\n",
    "    X_u_train_ic = np.hstack([x_u_ic, t_u_ic, logk_u_ic])\n",
    "\n",
    "    # --- Boundary conditions: u(0,t;k)=0, u(1,t;k)=0 ---\n",
    "    # We discretise t with 2*N_b points between t_min and t_max.\n",
    "    t_line = np.linspace(t_min, t_max, 2 * N_b)\n",
    "    x_bc_left  = np.array([x_min])\n",
    "    x_bc_right = np.array([x_max])\n",
    "    x_bc = np.concatenate([x_bc_left, x_bc_right], axis=0)  # [0, 1]\n",
    "\n",
    "    x_bc_g, t_bc_g, logk_bc_g = np.meshgrid(x_bc, t_line, logk_vec, indexing='ij')\n",
    "    x_u_bc    = x_bc_g.ravel()[:, None]\n",
    "    t_u_bc    = t_bc_g.ravel()[:, None]\n",
    "    logk_u_bc = logk_bc_g.ravel()[:, None]\n",
    "    X_u_train_bc = np.hstack([x_u_bc, t_u_bc, logk_u_bc])\n",
    "\n",
    "    # --- Combine IC and BC into one \"data\" set ---\n",
    "    X_u_train = np.vstack([X_u_train_ic, X_u_train_bc]).astype(np.float32)\n",
    "\n",
    "    # --- Analytic solution for those IC/BC points ---\n",
    "    x_cal    = X_u_train[:, 0]\n",
    "    t_cal    = X_u_train[:, 1]\n",
    "    logk_cal = X_u_train[:, 2]\n",
    "    k_cal    = 10.0**logk_cal\n",
    "\n",
    "    # Closed-form solution: u(x,t;k) = exp(-k π² t) sin(π x)\n",
    "    u_train = np.exp(-k_cal * (np.pi**2) * t_cal) * np.sin(np.pi * x_cal)\n",
    "    u_train = u_train[:, None].astype(np.float32)\n",
    "\n",
    "    # --- Collocation + validation via LHS in (x, t, logk) ---\n",
    "    lb = np.array([x_min, t_min, logk_min], dtype=np.float32)\n",
    "    ub = np.array([x_max, t_max, logk_max], dtype=np.float32)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    U_all = lhs(3, samples=N_f + N_val)   # Latin Hypercube in [0,1]^3\n",
    "    X_all = lb + (ub - lb) * U_all        # map to [lb, ub] in (x, t, logk)\n",
    "    X_f_train = X_all[:N_f]\n",
    "    X_val     = X_all[N_f:]\n",
    "\n",
    "    return X_u_train, u_train, X_f_train, X_val, lb, ub\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Compute global relative L2 error for a trained model at a fixed k\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_rel_L2(model,\n",
    "                   x_min, x_max,\n",
    "                   t_min, t_max,\n",
    "                   k_val=1.0,\n",
    "                   Nx=100, Nt=100,\n",
    "                   device=device):\n",
    "    \"\"\"\n",
    "    Compute global relative L2 error of the model solution against the analytic\n",
    "    solution on a regular (x,t) grid at a fixed physical k = k_val.\n",
    "\n",
    "    rel_L2 = ||u_pred - u_true||_2 / ||u_true||_2\n",
    "    \"\"\"\n",
    "\n",
    "    # Build regular grid in x, t\n",
    "    x_test = np.linspace(x_min, x_max, Nx)\n",
    "    t_test = np.linspace(t_min, t_max, Nt)\n",
    "    logk_val = np.log10(k_val)  # convert to log10(k) for NN input\n",
    "\n",
    "    T, X = np.meshgrid(t_test, x_test, indexing='ij')  # shape (Nt, Nx)\n",
    "    LOGK = np.full_like(T, logk_val)                   # broadcast log10(k_val)\n",
    "\n",
    "    # Flatten to (Nt*Nx, 1) column vectors\n",
    "    x_flat    = X.ravel()[:, None]\n",
    "    t_flat    = T.ravel()[:, None]\n",
    "    logk_flat = LOGK.ravel()[:, None]\n",
    "\n",
    "    # Stack into (Nt*Nx, 3) array and convert to torch tensor\n",
    "    X_star    = np.hstack([x_flat, t_flat, logk_flat]).astype(np.float32)\n",
    "    X_star_tf = torch.from_numpy(X_star).to(device)\n",
    "\n",
    "    # NN prediction over the grid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        u_pred = model(X_star_tf).squeeze(1).cpu().numpy().reshape(T.shape)\n",
    "\n",
    "    # Analytic solution on the same grid\n",
    "    u_true = u_true_numpy(X, T, LOGK)\n",
    "\n",
    "    # Global relative L2 error\n",
    "    num = np.linalg.norm(u_pred - u_true)\n",
    "    den = np.linalg.norm(u_true)\n",
    "    rel_L2 = num / den if den > 0 else num\n",
    "\n",
    "    return rel_L2\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Plot training curves (loss, val MSE, max |error|) and save to file\n",
    "# -----------------------------------------------------------------------------\n",
    "def plot_training_curves(loss_values, mse_v_hist, max_errors,\n",
    "                         N_i, N_b, N_k, N_f,\n",
    "                         out_dir=\"sweep_results\"):\n",
    "    \"\"\"\n",
    "    Make the TrainLoss / ValMSE / Max|Error| vs epoch plot and save to file.\n",
    "\n",
    "    Args:\n",
    "        loss_values : list of train loss per epoch\n",
    "        mse_v_hist  : list of (epoch, val_MSE)\n",
    "        max_errors  : list of (epoch, max_abs_error) on validation\n",
    "        N_i, N_b, N_k, N_f : configuration used (for filename)\n",
    "        out_dir     : directory where the PNG is saved\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Training loss epochs\n",
    "    ep_train = range(len(loss_values))\n",
    "\n",
    "    # Validation MSE: unpack (epoch, mse)\n",
    "    ep_val  = [int(i) for i, _ in mse_v_hist]\n",
    "    mse_val = [\n",
    "        (m.detach().cpu().item() if torch.is_tensor(m) else float(m))\n",
    "        for _, m in mse_v_hist\n",
    "    ]\n",
    "\n",
    "    # Max absolute error: unpack (epoch, max_err)\n",
    "    ep_max   = [int(i) for i, _ in max_errors]\n",
    "    max_errs = [\n",
    "        (m.detach().cpu().item() if torch.is_tensor(m) else float(m))\n",
    "        for _, m in max_errors\n",
    "    ]\n",
    "\n",
    "    # Plot curves on log scale (since errors/loss typically span many orders)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(ep_train, loss_values, color='black', label='Train Loss')\n",
    "    plt.plot(ep_val,   mse_val,     color='red',   label='Validation MSE')\n",
    "    plt.plot(ep_max,   max_errs,    color='blue',  label='Max |Error| (Validation)')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss / Error')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Training Loss, Validation MSE, and Max Validation Error vs Iterations')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # File name tagged with the configuration\n",
    "    fname = f\"train_Ni{N_i}_Nb{N_b}_Nk{N_k}_Nf{N_f}.png\"\n",
    "    fpath = os.path.join(out_dir, fname)\n",
    "    plt.savefig(fpath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Run a single experiment for given (N_i, N_b, N_k, N_f)\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_single_experiment(N_i, N_b, N_k, N_f, seed,\n",
    "                          N_val=100,\n",
    "                          Train_epochs=100000,\n",
    "                          learning_rate=5e-4,\n",
    "                          k_val_eval=1.0,\n",
    "                          results_dir=\"sweep_results\"):\n",
    "    \"\"\"\n",
    "    Runs one full experiment:\n",
    "      - builds dataset for given (N_i, N_b, N_k, N_f)\n",
    "      - trains a fresh model\n",
    "      - saves training curve plot\n",
    "      - computes global rel L2 error at k = k_val_eval\n",
    "      - returns a dict with all requested info\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Build dataset (IC+BC data, collocation, validation, bounds)\n",
    "    X_u_train, u_train, X_f_train, X_val, lb, ub = build_dataset(\n",
    "        N_i=N_i, N_b=N_b, N_k=N_k, N_f=N_f, N_val=N_val,\n",
    "        x_min=x_min, x_max=x_max, t_min=t_min, t_max=t_max,\n",
    "        k_min=k_min, k_max=k_max, seed=seed\n",
    "    )\n",
    "\n",
    "    # 2) Initialise a new PINN model for this dataset\n",
    "    model = NeuralNet(layers, lb, ub).to(device).float()\n",
    "\n",
    "    # 3) Train and measure total wall-clock time for this configuration\n",
    "    exp_start = time.time()\n",
    "    loss_values, mse_v_hist, max_errors, best_ep, best_TL, best_v, best_max_err = train(\n",
    "        Train_epochs,\n",
    "        X_u_train,\n",
    "        u_train,\n",
    "        X_f_train,\n",
    "        X_val,\n",
    "        model,\n",
    "        learning_rate\n",
    "    )\n",
    "    total_elapsed = time.time() - exp_start\n",
    "\n",
    "    # 4) Compute global relative L2 error at a chosen k (e.g. k = 1.0)\n",
    "    rel_L2 = compute_rel_L2(\n",
    "        model,\n",
    "        x_min=x_min, x_max=x_max,\n",
    "        t_min=t_min, t_max=t_max,\n",
    "        k_val=k_val_eval,\n",
    "        Nx=100, Nt=100,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 5) Create and save the training curve plot for this run\n",
    "    curve_path = plot_training_curves(\n",
    "        loss_values, mse_v_hist, max_errors,\n",
    "        N_i=N_i, N_b=N_b, N_k=N_k, N_f=N_f,\n",
    "        out_dir=results_dir\n",
    "    )\n",
    "\n",
    "    # 6) Pack all information into a record dictionary\n",
    "    record = {\n",
    "        \"N_i\": N_i,\n",
    "        \"N_b\": N_b,\n",
    "        \"N_k\": N_k,\n",
    "        \"N_f\": N_f,\n",
    "        \"total_elapsed\": total_elapsed,\n",
    "        \"best_ep\": best_ep,\n",
    "        \"best_TL\": best_TL,\n",
    "        \"best_v\": best_v,\n",
    "        \"best_max_err\": best_max_err,\n",
    "        \"rel_L2\": rel_L2,\n",
    "        \"loss_values\": loss_values,\n",
    "        \"mse_v_hist\": mse_v_hist,\n",
    "        \"max_errors\": max_errors,\n",
    "        \"training_curve_path\": curve_path,\n",
    "    }\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Sweep settings and loops for N_f, N_i, N_b, N_k\n",
    "# -----------------------------------------------------------------------------\n",
    "layers = [3, 50, 50, 50, 1]\n",
    "x_min=0.0\n",
    "x_max=1.0\n",
    "t_min=0.0\n",
    "t_max=0.25\n",
    "k_min=0.2\n",
    "k_max=2.0\n",
    "seed=123\n",
    "    \n",
    "# Base values (same as your current defaults)\n",
    "BASE_N_i = 101\n",
    "BASE_N_b = 51\n",
    "BASE_N_k = 51\n",
    "BASE_N_f = 1000\n",
    "\n",
    "# Training hyperparameters for all sweeps\n",
    "Train_epochs = 100000\n",
    "learning_rate = 0.0005\n",
    "N_val = 100          # number of validation points from LHS\n",
    "k_val_eval = 1.0     # k at which global rel L2 is computed\n",
    "\n",
    "# ==========================\n",
    "# Sweep 1: Collocation points N_f\n",
    "# ==========================\n",
    "Nf_list = [500, 1000, 2000, 4000]  # values to test for N_f\n",
    "\n",
    "results_Nf = []\n",
    "\n",
    "for N_f in Nf_list:\n",
    "    print(f\"\\n=== Sweep N_f = {N_f} (N_i={BASE_N_i}, N_b={BASE_N_b}, N_k={BASE_N_k}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=BASE_N_i,\n",
    "        N_b=BASE_N_b,\n",
    "        N_k=BASE_N_k,\n",
    "        N_f=N_f,\n",
    "        seed=123,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Nf\",\n",
    "    )\n",
    "    results_Nf.append(rec)\n",
    "\n",
    "# Convert list of records to DataFrame for easy inspection and saving\n",
    "df_Nf = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        # Histories stored as objects (lists) – still useful inside Python\n",
    "        # \"loss_values\": r[\"loss_values\"],\n",
    "        # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        # \"max_errors\": r[\"max_errors\"],\n",
    "        # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Nf\n",
    "])\n",
    "\n",
    "print(\"\\nN_f sweep summary:\")\n",
    "display(df_Nf)\n",
    "\n",
    "# Save N_f sweep summary table as Excel\n",
    "df_Nf.to_excel(\"sweep_Nf_summary.xlsx\", index=False)\n",
    "\n",
    "# ==========================\n",
    "# Sweep 2: IC points N_i\n",
    "# ==========================\n",
    "Ni_list = [21, 51, 101, 201]  # values to test for N_i\n",
    "\n",
    "results_Ni = []\n",
    "\n",
    "for N_i in Ni_list:\n",
    "    print(f\"\\n=== Sweep N_i = {N_i} (N_b={BASE_N_b}, N_k={BASE_N_k}, N_f={BASE_N_f}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=N_i,\n",
    "        N_b=BASE_N_b,\n",
    "        N_k=BASE_N_k,\n",
    "        N_f=BASE_N_f,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Ni\",\n",
    "        seed=123\n",
    "    )\n",
    "    results_Ni.append(rec)\n",
    "\n",
    "df_Ni = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        # \"loss_values\": r[\"loss_values\"],\n",
    "        # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        # \"max_errors\": r[\"max_errors\"],\n",
    "        # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Ni\n",
    "])\n",
    "\n",
    "print(\"\\nN_i sweep summary:\")\n",
    "display(df_Ni)\n",
    "\n",
    "df_Ni.to_excel(\"sweep_Ni_summary.xlsx\", index=False)\n",
    "\n",
    "# ==========================\n",
    "# Sweep 3: BC points N_b\n",
    "# ==========================\n",
    "Nb_list = [11, 21, 51, 101]  # values to test for N_b\n",
    "\n",
    "results_Nb = []\n",
    "\n",
    "for N_b in Nb_list:\n",
    "    print(f\"\\n=== Sweep N_b = {N_b} (N_i={BASE_N_i}, N_k={BASE_N_k}, N_f={BASE_N_f}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=BASE_N_i,\n",
    "        N_b=N_b,\n",
    "        N_k=BASE_N_k,\n",
    "        N_f=BASE_N_f,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Nb\",\n",
    "        seed=123\n",
    "    )\n",
    "    results_Nb.append(rec)\n",
    "\n",
    "df_Nb = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        # \"loss_values\": r[\"loss_values\"],\n",
    "        # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        # \"max_errors\": r[\"max_errors\"],\n",
    "        # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Nb\n",
    "])\n",
    "\n",
    "print(\"\\nN_b sweep summary:\")\n",
    "display(df_Nb)\n",
    "\n",
    "df_Nb.to_excel(\"sweep_Nb_summary.xlsx\", index=False)\n",
    "\n",
    "# ==========================\n",
    "# Sweep 4: parameter samples N_k\n",
    "# ==========================\n",
    "Nk_list = [11, 21, 51, 101]  # values to test for N_k\n",
    "\n",
    "results_Nk = []\n",
    "\n",
    "for N_k in Nk_list:\n",
    "    print(f\"\\n=== Sweep N_k = {N_k} (N_i={BASE_N_i}, N_b={BASE_N_b}, N_f={BASE_N_f}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=BASE_N_i,\n",
    "        N_b=BASE_N_b,\n",
    "        N_k=N_k,\n",
    "        N_f=BASE_N_f,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Nk\",\n",
    "        seed=123\n",
    "    )\n",
    "    results_Nk.append(rec)\n",
    "\n",
    "df_Nk = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        # \"loss_values\": r[\"loss_values\"],\n",
    "        # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        # \"max_errors\": r[\"max_errors\"],\n",
    "        # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Nk\n",
    "])\n",
    "\n",
    "print(\"\\nN_k sweep summary:\")\n",
    "display(df_Nk)\n",
    "\n",
    "df_Nk.to_excel(\"sweep_Nk_summary.xlsx\", index=False)\n",
    "\n",
    "print(\"\\n=== END OF SCRIPT ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758839791644,
     "user": {
      "displayName": "Oscar Zhang",
      "userId": "08884929965206842440"
     },
     "user_tz": -60
    },
    "id": "jQWrqMPo5Apy"
   },
   "outputs": [],
   "source": [
    "# t = data['t'].flatten()[:,None] # read in t and flatten into column vector\n",
    "# x = data['x'].flatten()[:,None] # read in x and flatten into column vector\n",
    "#  # Exact represents the exact solution to the problem, from the data provided\n",
    "# Exact = np.real(data['usol']).T # Exact has structure of nt times nx\n",
    "\n",
    "# print(\"usol shape (nt, nx) = \", Exact.shape)\n",
    "\n",
    "# # We need to find all the x,t coordinate pairs in the domain\n",
    "# X, T = np.meshgrid(x,t)\n",
    "\n",
    "# # Flatten the coordinate grid into pairs of x,t coordinates\n",
    "# X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # coordinates x,t\n",
    "# u_star = Exact.flatten()[:,None]   # corresponding solution value with each coordinate\n",
    "\n",
    "\n",
    "# print(\"X has shape \", X.shape, \", X_star has shape \", X_star.shape, \", u_star has shape \", u_star.shape)\n",
    "\n",
    "# # Domain bounds (-1,1)\n",
    "# lb = X_star.min(axis=0)\n",
    "# ub = X_star.max(axis=0)\n",
    "\n",
    "# print(\"Lower bounds of x,t: \", lb)\n",
    "# print(\"Upper bounds of x,t: \", ub)\n",
    "# print('')\n",
    "# print('The first few entries of X_star are:')\n",
    "# print( X_star[0:5, :] )\n",
    "\n",
    "# print('')\n",
    "# print('The first few entries of u_star are:')\n",
    "# print( u_star[0:5, :] )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

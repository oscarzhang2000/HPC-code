{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1758839791622,
     "user": {
      "displayName": "Oscar Zhang",
      "userId": "08884929965206842440"
     },
     "user_tz": -60
    },
    "id": "fWGYfkhF4JUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? True\n",
      "Device: NVIDIA A2\n",
      "Using device: cuda\n",
      "\n",
      "=== Sweep N_f = 1000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 1.494e+01 | Val MSE: 1.009e-01 | Max Val |err|: 7.136e-01 | Time: 3.91s\n",
      "Epochs:   1000 | TrainLoss: 1.674e-01 | Val MSE: 2.935e-04 | Max Val |err|: 5.648e-02 | Time: 3.41s\n",
      "Epochs:   2000 | TrainLoss: 2.336e-02 | Val MSE: 3.724e-05 | Max Val |err|: 2.171e-02 | Time: 3.42s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2998 (TrainLoss=7.472e-03, Val MSE=1.550e-05)\n",
      "Epochs:   3000 | TrainLoss: 7.474e-03 | Val MSE: 1.312e-05 | Max Val |err|: 1.481e-02 | Time: 3.57s\n",
      "Epochs:   4000 | TrainLoss: 1.094e-05 | Val MSE: 2.652e-08 | Max Val |err|: 6.464e-04 | Time: 37.39s\n",
      "Epochs:   5000 | TrainLoss: 1.094e-05 | Val MSE: 2.652e-08 | Max Val |err|: 6.464e-04 | Time: 8.52s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 68.61 s\n",
      "Best Val MSE: 2.652e-08 at epoch 3193\n",
      "Best Max |err| on validation: 6.464e-04\n",
      "\n",
      "=== Sweep N_f = 10000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 6.097e+01 | Val MSE: 3.070e-01 | Max Val |err|: 1.580e+00 | Time: 0.01s\n",
      "Epochs:   1000 | TrainLoss: 3.576e-01 | Val MSE: 7.295e-04 | Max Val |err|: 1.410e-01 | Time: 7.40s\n",
      "Epochs:   2000 | TrainLoss: 9.735e-02 | Val MSE: 1.661e-04 | Max Val |err|: 5.501e-02 | Time: 7.40s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2989 (TrainLoss=1.100e-02, Val MSE=1.088e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.100e-02 | Val MSE: 7.462e-06 | Max Val |err|: 1.110e-02 | Time: 7.53s\n",
      "Epochs:   4000 | TrainLoss: 7.038e-06 | Val MSE: 8.685e-09 | Max Val |err|: 3.836e-04 | Time: 60.28s\n",
      "Epochs:   5000 | TrainLoss: 7.038e-06 | Val MSE: 8.685e-09 | Max Val |err|: 3.836e-04 | Time: 12.44s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 107.53 s\n",
      "Best Val MSE: 8.685e-09 at epoch 3231\n",
      "Best Max |err| on validation: 3.836e-04\n",
      "\n",
      "=== Sweep N_f = 50000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 5.441e+01 | Val MSE: 2.171e-01 | Max Val |err|: 1.192e+00 | Time: 0.14s\n",
      "Epochs:   1000 | TrainLoss: 2.670e-01 | Val MSE: 3.098e-04 | Max Val |err|: 6.877e-02 | Time: 28.40s\n",
      "Epochs:   2000 | TrainLoss: 4.738e-02 | Val MSE: 4.507e-05 | Max Val |err|: 2.208e-02 | Time: 28.46s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.227e-02, Val MSE=7.800e-06)\n",
      "Epochs:   3000 | TrainLoss: 1.226e-02 | Val MSE: 5.591e-06 | Max Val |err|: 7.376e-03 | Time: 28.92s\n",
      "Epochs:   4000 | TrainLoss: 7.837e-06 | Val MSE: 8.817e-09 | Max Val |err|: 4.060e-04 | Time: 146.86s\n",
      "Epochs:   5000 | TrainLoss: 7.837e-06 | Val MSE: 8.817e-09 | Max Val |err|: 4.060e-04 | Time: 33.44s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 299.59 s\n",
      "Best Val MSE: 8.817e-09 at epoch 3192\n",
      "Best Max |err| on validation: 4.060e-04\n",
      "\n",
      "=== Sweep N_f = 100000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 1.544e+01 | Val MSE: 1.355e-01 | Max Val |err|: 7.759e-01 | Time: 0.05s\n",
      "Epochs:   1000 | TrainLoss: 1.932e-01 | Val MSE: 3.445e-04 | Max Val |err|: 5.661e-02 | Time: 51.97s\n",
      "Epochs:   2000 | TrainLoss: 4.274e-02 | Val MSE: 3.626e-05 | Max Val |err|: 2.390e-02 | Time: 51.91s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=9.748e-03, Val MSE=9.158e-06)\n",
      "Epochs:   3000 | TrainLoss: 9.736e-03 | Val MSE: 8.110e-06 | Max Val |err|: 1.050e-02 | Time: 52.84s\n",
      "Epochs:   4000 | TrainLoss: 6.917e-06 | Val MSE: 9.089e-09 | Max Val |err|: 3.029e-04 | Time: 271.21s\n",
      "Epochs:   5000 | TrainLoss: 6.917e-06 | Val MSE: 9.089e-09 | Max Val |err|: 3.029e-04 | Time: 57.04s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 542.03 s\n",
      "Best Val MSE: 9.089e-09 at epoch 3215\n",
      "Best Max |err| on validation: 3.029e-04\n",
      "\n",
      "N_f sweep summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_i</th>\n",
       "      <th>N_b</th>\n",
       "      <th>N_k</th>\n",
       "      <th>N_f</th>\n",
       "      <th>total_elapsed</th>\n",
       "      <th>best_ep</th>\n",
       "      <th>best_TL</th>\n",
       "      <th>best_v</th>\n",
       "      <th>best_max_err</th>\n",
       "      <th>rel_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>84.456324</td>\n",
       "      <td>3193</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>2.651857e-08</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>10000</td>\n",
       "      <td>107.532178</td>\n",
       "      <td>3231</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>8.685141e-09</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>50000</td>\n",
       "      <td>299.601784</td>\n",
       "      <td>3192</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>8.817134e-09</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>100000</td>\n",
       "      <td>542.034473</td>\n",
       "      <td>3215</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>9.088913e-09</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N_i  N_b  N_k     N_f  total_elapsed  best_ep   best_TL        best_v  \\\n",
       "0  101   51   51    1000      84.456324     3193  0.000011  2.651857e-08   \n",
       "1  101   51   51   10000     107.532178     3231  0.000007  8.685141e-09   \n",
       "2  101   51   51   50000     299.601784     3192  0.000008  8.817134e-09   \n",
       "3  101   51   51  100000     542.034473     3215  0.000007  9.088913e-09   \n",
       "\n",
       "   best_max_err    rel_L2  \n",
       "0      0.000646  0.000370  \n",
       "1      0.000384  0.000222  \n",
       "2      0.000406  0.000262  \n",
       "3      0.000303  0.000270  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sweep N_i = 101 (N_b=51, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 2.064e+01 | Val MSE: 1.349e-01 | Max Val |err|: 7.544e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 1.543e-01 | Val MSE: 2.203e-04 | Max Val |err|: 5.226e-02 | Time: 3.45s\n",
      "Epochs:   2000 | TrainLoss: 3.034e-02 | Val MSE: 4.348e-05 | Max Val |err|: 2.475e-02 | Time: 3.31s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2996 (TrainLoss=5.849e-03, Val MSE=8.867e-06)\n",
      "Epochs:   3000 | TrainLoss: 5.992e-03 | Val MSE: 5.763e-06 | Max Val |err|: 1.091e-02 | Time: 3.32s\n",
      "Epochs:   4000 | TrainLoss: 9.058e-06 | Val MSE: 3.013e-08 | Max Val |err|: 1.093e-03 | Time: 41.48s\n",
      "Epochs:   5000 | TrainLoss: 9.058e-06 | Val MSE: 3.013e-08 | Max Val |err|: 1.093e-03 | Time: 8.41s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 68.45 s\n",
      "Best Val MSE: 3.013e-08 at epoch 3237\n",
      "Best Max |err| on validation: 1.093e-03\n",
      "\n",
      "=== Sweep N_i = 1001 (N_b=51, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 2.744e+01 | Val MSE: 1.112e-01 | Max Val |err|: 7.357e-01 | Time: 0.01s\n",
      "Epochs:   1000 | TrainLoss: 2.529e-01 | Val MSE: 6.933e-04 | Max Val |err|: 7.115e-02 | Time: 5.65s\n",
      "Epochs:   2000 | TrainLoss: 4.846e-02 | Val MSE: 1.795e-04 | Max Val |err|: 3.217e-02 | Time: 5.59s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.634e-02, Val MSE=4.410e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.632e-02 | Val MSE: 2.914e-05 | Max Val |err|: 1.670e-02 | Time: 5.68s\n",
      "Epochs:   4000 | TrainLoss: 1.048e-05 | Val MSE: 6.080e-08 | Max Val |err|: 9.857e-04 | Time: 54.16s\n",
      "Epochs:   5000 | TrainLoss: 1.048e-05 | Val MSE: 6.080e-08 | Max Val |err|: 9.857e-04 | Time: 10.65s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 92.37 s\n",
      "Best Val MSE: 6.080e-08 at epoch 3241\n",
      "Best Max |err| on validation: 9.857e-04\n",
      "\n",
      "=== Sweep N_i = 5001 (N_b=51, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 3.583e+01 | Val MSE: 1.018e-01 | Max Val |err|: 6.949e-01 | Time: 0.02s\n",
      "Epochs:   1000 | TrainLoss: 2.123e-01 | Val MSE: 2.132e-03 | Max Val |err|: 1.086e-01 | Time: 20.01s\n",
      "Epochs:   2000 | TrainLoss: 3.944e-02 | Val MSE: 1.030e-03 | Max Val |err|: 8.197e-02 | Time: 20.10s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.288e-02, Val MSE=4.001e-04)\n",
      "Epochs:   3000 | TrainLoss: 1.285e-02 | Val MSE: 4.037e-04 | Max Val |err|: 5.908e-02 | Time: 20.45s\n",
      "Epochs:   4000 | TrainLoss: 1.183e-05 | Val MSE: 3.470e-07 | Max Val |err|: 2.499e-03 | Time: 106.18s\n",
      "Epochs:   5000 | TrainLoss: 1.183e-05 | Val MSE: 3.470e-07 | Max Val |err|: 2.499e-03 | Time: 24.93s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 216.58 s\n",
      "Best Val MSE: 3.470e-07 at epoch 3186\n",
      "Best Max |err| on validation: 2.499e-03\n",
      "\n",
      "=== Sweep N_i = 10001 (N_b=51, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 7.604e+01 | Val MSE: 1.220e-01 | Max Val |err|: 9.875e-01 | Time: 0.04s\n",
      "Epochs:   1000 | TrainLoss: 3.399e-01 | Val MSE: 9.779e-03 | Max Val |err|: 2.229e-01 | Time: 35.59s\n",
      "Epochs:   2000 | TrainLoss: 8.092e-02 | Val MSE: 4.495e-03 | Max Val |err|: 1.734e-01 | Time: 35.60s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=2.111e-02, Val MSE=1.668e-03)\n",
      "Epochs:   3000 | TrainLoss: 2.108e-02 | Val MSE: 1.347e-03 | Max Val |err|: 1.202e-01 | Time: 36.26s\n",
      "Epochs:   4000 | TrainLoss: 1.422e-05 | Val MSE: 9.576e-07 | Max Val |err|: 5.550e-03 | Time: 223.41s\n",
      "Epochs:   5000 | TrainLoss: 1.422e-05 | Val MSE: 9.576e-07 | Max Val |err|: 5.550e-03 | Time: 40.37s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 411.61 s\n",
      "Best Val MSE: 9.576e-07 at epoch 3251\n",
      "Best Max |err| on validation: 5.550e-03\n",
      "\n",
      "N_i sweep summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_i</th>\n",
       "      <th>N_b</th>\n",
       "      <th>N_k</th>\n",
       "      <th>N_f</th>\n",
       "      <th>total_elapsed</th>\n",
       "      <th>best_ep</th>\n",
       "      <th>best_TL</th>\n",
       "      <th>best_v</th>\n",
       "      <th>best_max_err</th>\n",
       "      <th>rel_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>68.447227</td>\n",
       "      <td>3237</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3.012899e-08</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>92.377010</td>\n",
       "      <td>3241</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>6.079636e-08</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.000512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5001</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>216.583314</td>\n",
       "      <td>3186</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>3.470103e-07</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.001702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>411.617809</td>\n",
       "      <td>3251</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>9.576163e-07</td>\n",
       "      <td>0.005550</td>\n",
       "      <td>0.002836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     N_i  N_b  N_k   N_f  total_elapsed  best_ep   best_TL        best_v  \\\n",
       "0    101   51   51  1000      68.447227     3237  0.000009  3.012899e-08   \n",
       "1   1001   51   51  1000      92.377010     3241  0.000010  6.079636e-08   \n",
       "2   5001   51   51  1000     216.583314     3186  0.000012  3.470103e-07   \n",
       "3  10001   51   51  1000     411.617809     3251  0.000014  9.576163e-07   \n",
       "\n",
       "   best_max_err    rel_L2  \n",
       "0      0.001093  0.000255  \n",
       "1      0.000986  0.000512  \n",
       "2      0.002499  0.001702  \n",
       "3      0.005550  0.002836  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sweep N_b = 51 (N_i=101, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 1.710e+01 | Val MSE: 1.044e-01 | Max Val |err|: 7.530e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 1.870e-01 | Val MSE: 3.332e-04 | Max Val |err|: 6.003e-02 | Time: 3.56s\n",
      "Epochs:   2000 | TrainLoss: 2.428e-02 | Val MSE: 4.080e-05 | Max Val |err|: 3.012e-02 | Time: 3.47s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2998 (TrainLoss=8.408e-03, Val MSE=1.648e-05)\n",
      "Epochs:   3000 | TrainLoss: 8.417e-03 | Val MSE: 1.414e-05 | Max Val |err|: 2.036e-02 | Time: 3.35s\n",
      "Epochs:   4000 | TrainLoss: 7.898e-06 | Val MSE: 1.174e-08 | Max Val |err|: 3.568e-04 | Time: 37.73s\n",
      "Epochs:   5000 | TrainLoss: 7.898e-06 | Val MSE: 1.174e-08 | Max Val |err|: 3.568e-04 | Time: 8.50s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 65.04 s\n",
      "Best Val MSE: 1.174e-08 at epoch 3212\n",
      "Best Max |err| on validation: 3.568e-04\n",
      "\n",
      "=== Sweep N_b = 501 (N_i=101, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 1.334e+01 | Val MSE: 1.147e-01 | Max Val |err|: 6.408e-01 | Time: 0.01s\n",
      "Epochs:   1000 | TrainLoss: 6.545e-02 | Val MSE: 1.530e-04 | Max Val |err|: 4.600e-02 | Time: 8.48s\n",
      "Epochs:   2000 | TrainLoss: 1.687e-02 | Val MSE: 5.420e-05 | Max Val |err|: 4.186e-02 | Time: 8.38s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2972 (TrainLoss=8.370e-03, Val MSE=2.968e-05)\n",
      "Epochs:   3000 | TrainLoss: 8.371e-03 | Val MSE: 3.403e-05 | Max Val |err|: 2.137e-02 | Time: 8.56s\n",
      "Epochs:   4000 | TrainLoss: 8.839e-06 | Val MSE: 1.971e-08 | Max Val |err|: 6.346e-04 | Time: 59.76s\n",
      "Epochs:   5000 | TrainLoss: 8.839e-06 | Val MSE: 1.971e-08 | Max Val |err|: 6.346e-04 | Time: 13.54s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 112.24 s\n",
      "Best Val MSE: 1.971e-08 at epoch 3202\n",
      "Best Max |err| on validation: 6.346e-04\n",
      "\n",
      "=== Sweep N_b = 2501 (N_i=101, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 1.899e+01 | Val MSE: 1.977e-01 | Max Val |err|: 8.333e-01 | Time: 0.03s\n",
      "Epochs:   1000 | TrainLoss: 2.558e-01 | Val MSE: 4.082e-02 | Max Val |err|: 5.404e-01 | Time: 35.36s\n",
      "Epochs:   2000 | TrainLoss: 3.067e-02 | Val MSE: 9.360e-04 | Max Val |err|: 1.362e-01 | Time: 35.38s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=8.237e-03, Val MSE=2.291e-04)\n",
      "Epochs:   3000 | TrainLoss: 8.228e-03 | Val MSE: 3.004e-04 | Max Val |err|: 5.736e-02 | Time: 35.97s\n",
      "Epochs:   4000 | TrainLoss: 8.641e-06 | Val MSE: 4.092e-08 | Max Val |err|: 6.176e-04 | Time: 123.30s\n",
      "Epochs:   5000 | TrainLoss: 8.641e-06 | Val MSE: 4.092e-08 | Max Val |err|: 6.176e-04 | Time: 40.09s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 310.19 s\n",
      "Best Val MSE: 4.092e-08 at epoch 3118\n",
      "Best Max |err| on validation: 6.176e-04\n",
      "\n",
      "=== Sweep N_b = 5001 (N_i=101, N_k=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 2.482e+00 | Val MSE: 9.851e-02 | Max Val |err|: 8.259e-01 | Time: 0.07s\n",
      "Epochs:   1000 | TrainLoss: 1.214e-01 | Val MSE: 3.456e-02 | Max Val |err|: 4.970e-01 | Time: 73.25s\n",
      "Epochs:   2000 | TrainLoss: 2.962e-02 | Val MSE: 2.594e-03 | Max Val |err|: 2.048e-01 | Time: 73.15s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.315e-02, Val MSE=7.964e-04)\n",
      "Epochs:   3000 | TrainLoss: 1.314e-02 | Val MSE: 6.724e-04 | Max Val |err|: 1.226e-01 | Time: 74.52s\n",
      "Epochs:   4000 | TrainLoss: 8.845e-06 | Val MSE: 1.259e-07 | Max Val |err|: 1.953e-03 | Time: 347.97s\n",
      "Epochs:   5000 | TrainLoss: 8.845e-06 | Val MSE: 1.259e-07 | Max Val |err|: 1.953e-03 | Time: 78.02s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 724.94 s\n",
      "Best Val MSE: 1.259e-07 at epoch 3202\n",
      "Best Max |err| on validation: 1.953e-03\n",
      "\n",
      "N_b sweep summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_i</th>\n",
       "      <th>N_b</th>\n",
       "      <th>N_k</th>\n",
       "      <th>N_f</th>\n",
       "      <th>total_elapsed</th>\n",
       "      <th>best_ep</th>\n",
       "      <th>best_TL</th>\n",
       "      <th>best_v</th>\n",
       "      <th>best_max_err</th>\n",
       "      <th>rel_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>65.044047</td>\n",
       "      <td>3212</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.173609e-08</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>501</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>112.243549</td>\n",
       "      <td>3202</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1.971152e-08</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.000295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2501</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>310.194026</td>\n",
       "      <td>3118</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.092134e-08</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>5001</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>724.949265</td>\n",
       "      <td>3202</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1.259180e-07</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N_i   N_b  N_k   N_f  total_elapsed  best_ep   best_TL        best_v  \\\n",
       "0  101    51   51  1000      65.044047     3212  0.000008  1.173609e-08   \n",
       "1  101   501   51  1000     112.243549     3202  0.000009  1.971152e-08   \n",
       "2  101  2501   51  1000     310.194026     3118  0.000009  4.092134e-08   \n",
       "3  101  5001   51  1000     724.949265     3202  0.000009  1.259180e-07   \n",
       "\n",
       "   best_max_err    rel_L2  \n",
       "0      0.000357  0.000294  \n",
       "1      0.000635  0.000295  \n",
       "2      0.000618  0.000405  \n",
       "3      0.001953  0.001087  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sweep N_k = 51 (N_i=101, N_b=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 2.772e+01 | Val MSE: 1.579e-01 | Max Val |err|: 6.754e-01 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 1.733e-01 | Val MSE: 2.632e-04 | Max Val |err|: 5.406e-02 | Time: 3.40s\n",
      "Epochs:   2000 | TrainLoss: 3.197e-02 | Val MSE: 6.253e-05 | Max Val |err|: 2.855e-02 | Time: 3.36s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.041e-02, Val MSE=2.063e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.040e-02 | Val MSE: 1.861e-05 | Max Val |err|: 1.588e-02 | Time: 3.37s\n",
      "Epochs:   4000 | TrainLoss: 9.702e-06 | Val MSE: 1.588e-08 | Max Val |err|: 6.487e-04 | Time: 36.77s\n",
      "Epochs:   5000 | TrainLoss: 9.702e-06 | Val MSE: 1.588e-08 | Max Val |err|: 6.487e-04 | Time: 8.42s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 63.88 s\n",
      "Best Val MSE: 1.588e-08 at epoch 3198\n",
      "Best Max |err| on validation: 6.487e-04\n",
      "\n",
      "=== Sweep N_k = 501 (N_i=101, N_b=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 2.796e+01 | Val MSE: 1.311e-01 | Max Val |err|: 8.418e-01 | Time: 0.01s\n",
      "Epochs:   1000 | TrainLoss: 2.041e-01 | Val MSE: 2.990e-04 | Max Val |err|: 5.190e-02 | Time: 12.10s\n",
      "Epochs:   2000 | TrainLoss: 2.284e-02 | Val MSE: 4.322e-05 | Max Val |err|: 3.409e-02 | Time: 12.11s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.198e-02, Val MSE=2.682e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.198e-02 | Val MSE: 2.299e-05 | Max Val |err|: 2.706e-02 | Time: 12.30s\n",
      "Epochs:   4000 | TrainLoss: 1.511e-05 | Val MSE: 5.675e-08 | Max Val |err|: 1.207e-03 | Time: 59.19s\n",
      "Epochs:   5000 | TrainLoss: 1.511e-05 | Val MSE: 5.675e-08 | Max Val |err|: 1.207e-03 | Time: 17.08s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 129.86 s\n",
      "Best Val MSE: 5.675e-08 at epoch 3138\n",
      "Best Max |err| on validation: 1.207e-03\n",
      "\n",
      "=== Sweep N_k = 2501 (N_i=101, N_b=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 2.708e+01 | Val MSE: 1.446e-01 | Max Val |err|: 7.652e-01 | Time: 0.05s\n",
      "Epochs:   1000 | TrainLoss: 2.102e-01 | Val MSE: 3.201e-04 | Max Val |err|: 6.225e-02 | Time: 51.66s\n",
      "Epochs:   2000 | TrainLoss: 2.252e-02 | Val MSE: 4.282e-05 | Max Val |err|: 3.059e-02 | Time: 51.62s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2996 (TrainLoss=8.516e-03, Val MSE=1.630e-05)\n",
      "Epochs:   3000 | TrainLoss: 8.579e-03 | Val MSE: 1.207e-05 | Max Val |err|: 1.216e-02 | Time: 52.41s\n",
      "Epochs:   4000 | TrainLoss: 8.077e-06 | Val MSE: 1.371e-08 | Max Val |err|: 5.380e-04 | Time: 285.38s\n",
      "Epochs:   5000 | TrainLoss: 8.077e-06 | Val MSE: 1.371e-08 | Max Val |err|: 5.380e-04 | Time: 56.22s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 553.51 s\n",
      "Best Val MSE: 1.371e-08 at epoch 3230\n",
      "Best Max |err| on validation: 5.380e-04\n",
      "\n",
      "=== Sweep N_k = 5001 (N_i=101, N_b=51, N_f=1000) ===\n",
      "Epochs:      0 | TrainLoss: 2.995e+01 | Val MSE: 1.675e-01 | Max Val |err|: 8.483e-01 | Time: 0.10s\n",
      "Epochs:   1000 | TrainLoss: 1.619e-01 | Val MSE: 2.258e-04 | Max Val |err|: 5.598e-02 | Time: 106.43s\n",
      "Epochs:   2000 | TrainLoss: 4.759e-02 | Val MSE: 8.644e-05 | Max Val |err|: 2.783e-02 | Time: 106.71s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.429e-02, Val MSE=2.526e-05)\n",
      "Epochs:   3000 | TrainLoss: 1.434e-02 | Val MSE: 1.950e-05 | Max Val |err|: 2.084e-02 | Time: 108.25s\n",
      "Epochs:   4000 | TrainLoss: 9.145e-06 | Val MSE: 2.619e-08 | Max Val |err|: 6.751e-04 | Time: 576.02s\n",
      "Epochs:   5000 | TrainLoss: 9.145e-06 | Val MSE: 2.619e-08 | Max Val |err|: 6.751e-04 | Time: 110.68s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 1118.77 s\n",
      "Best Val MSE: 2.619e-08 at epoch 3245\n",
      "Best Max |err| on validation: 6.751e-04\n",
      "\n",
      "N_k sweep summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_i</th>\n",
       "      <th>N_b</th>\n",
       "      <th>N_k</th>\n",
       "      <th>N_f</th>\n",
       "      <th>total_elapsed</th>\n",
       "      <th>best_ep</th>\n",
       "      <th>best_TL</th>\n",
       "      <th>best_v</th>\n",
       "      <th>best_max_err</th>\n",
       "      <th>rel_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>63.878303</td>\n",
       "      <td>3198</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.588432e-08</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>501</td>\n",
       "      <td>1000</td>\n",
       "      <td>129.865805</td>\n",
       "      <td>3138</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>5.674530e-08</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.000425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>2501</td>\n",
       "      <td>1000</td>\n",
       "      <td>553.511057</td>\n",
       "      <td>3230</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.370849e-08</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>5001</td>\n",
       "      <td>1000</td>\n",
       "      <td>1118.804537</td>\n",
       "      <td>3245</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2.618719e-08</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.000319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N_i  N_b   N_k   N_f  total_elapsed  best_ep   best_TL        best_v  \\\n",
       "0  101   51    51  1000      63.878303     3198  0.000010  1.588432e-08   \n",
       "1  101   51   501  1000     129.865805     3138  0.000015  5.674530e-08   \n",
       "2  101   51  2501  1000     553.511057     3230  0.000008  1.370849e-08   \n",
       "3  101   51  5001  1000    1118.804537     3245  0.000009  2.618719e-08   \n",
       "\n",
       "   best_max_err    rel_L2  \n",
       "0      0.000649  0.000293  \n",
       "1      0.001207  0.000425  \n",
       "2      0.000538  0.000302  \n",
       "3      0.000675  0.000319  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== END OF SCRIPT ===\n"
     ]
    }
   ],
   "source": [
    "# For readability: disable warnings from libraries like matplotlib, etc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "# Make sure torch is imported somewhere above this cell:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "from itertools import product, combinations\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as la\n",
    "from pyDOE import lhs\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogLocator, FuncFormatter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# --- Device Setup ---\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Select the most performant device available (CUDA > MPS > CPU)\n",
    "device = (\n",
    "    torch.device('cuda') if torch.cuda.is_available()\n",
    "    else torch.device('mps') if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else torch.device('cpu')\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "def u_true_numpy(X, T, logK):\n",
    "    \"\"\"Vectorised true solution: U = exp(-(pi^2) * T) * sin(pi * X).\"\"\"\n",
    "    K = 10.0**logK    # convert log10(k) → k\n",
    "    return np.exp(- K * (np.pi**2) * T) * np.sin(np.pi * X)\n",
    "\n",
    "def net_u(x, t, logk, model):\n",
    "    \"\"\"\n",
    "    NN input is (x, t, log10(k)).\n",
    "    \"\"\"\n",
    "    X = torch.cat([x, t, logk], dim=1)  # If x and t are each shape (N, 1), then X becomes (N, 2).\n",
    "    u = model(X)\n",
    "    return u\n",
    "\n",
    "# net_f computes the PDE residual\n",
    "# If f ≈ 0 at collocation points, the NN satisfies the equation there\n",
    "def net_f(x, t, logk, model):\n",
    "    x.requires_grad_(True)\n",
    "    t.requires_grad_(True)\n",
    "    # logk.requires_grad_(True)\n",
    "    \n",
    "    u = net_u(x, t, logk, model)\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    \n",
    "    # convert log10(k) -> k = 10^logk\n",
    "    k = 10.0**logk\n",
    "    \n",
    "    f = u_t - k * u_xx\n",
    "    return f\n",
    "\n",
    "class XavierInit(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(XavierInit, self).__init__()\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = torch.sqrt(torch.tensor(2.0 / (in_dim + out_dim)))\n",
    "        self.weight = nn.Parameter(torch.randn(in_dim, out_dim) * xavier_stddev)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "def initialize_NN(layers):\n",
    "    weights = nn.ModuleList()\n",
    "    num_layers = len(layers)\n",
    "    for l in range(num_layers - 1):\n",
    "        layer = XavierInit(size=[layers[l], layers[l + 1]]) # if there was no retutn, how do I get the weight and bias?\n",
    "        weights.append(layer)\n",
    "    return weights\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, layers, lb, ub):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.weights = initialize_NN(layers)\n",
    "        # make lb/ub move with .to(device)\n",
    "        self.register_buffer('lb', torch.as_tensor(lb, dtype=torch.float32))     # <<< CHANGED >>>\n",
    "        self.register_buffer('ub', torch.as_tensor(ub, dtype=torch.float32))     # <<< CHANGED >>>\n",
    "        # self.register_buffer('k', torch.tensor(k_init, dtype=torch.float32))     # <<< CHANGED >>>\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.float()                                                            # <<< CHANGED >>>\n",
    "        lb = self.lb.to(X.device)                                                # <<< CHANGED >>>\n",
    "        ub = self.ub.to(X.device)                                                # <<< CHANGED >>>\n",
    "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        for l in range(len(self.weights) - 1):\n",
    "            H = torch.tanh(self.weights[l](H.float()))     # Is this already a calculation?\n",
    "        Y = self.weights[-1](H)\n",
    "        return Y\n",
    "\n",
    "def train(nEpoch, X, u, X_f, X_val, model, learning_rate):\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # ----- STAGE 1: start with Adam -----\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # when to switch from Adam to L-BFGS\n",
    "    switch_epoch = 3000.0\n",
    "    used_lbfgs   = False   \n",
    "    lbfgs_epochs   = 3000.0           # <-- how many epochs of L-BFGS you want\n",
    "    lbfgs_start_ep = None             # <-- will store the epoch where we switch\n",
    "\n",
    "    # use the model's device\n",
    "    dev = next(model.parameters()).device                                        # <<< CHANGED >>>\n",
    "\n",
    "    x    = X[:, 0:1]\n",
    "    t    = X[:, 1:2]\n",
    "    logk = X[:, 2:3]\n",
    "    # Collocation points (f points)\n",
    "    x_f    = X_f[:, 0:1]\n",
    "    t_f    = X_f[:, 1:2]\n",
    "    logk_f = X_f[:, 2:3]\n",
    "    # Validation points\n",
    "    x_v    = X_val[:, 0:1]\n",
    "    t_v    = X_val[:, 1:2]\n",
    "    logk_v = X_val[:, 2:3]\n",
    "\n",
    "    # True validation solution (analytic)\n",
    "    u_v_true = u_true_numpy(x_v, t_v, logk_v)   # shape (N_val,)\n",
    "\n",
    "    # create tensors ON THE SAME DEVICE\n",
    "    x_tf      = torch.tensor(x,        dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    t_tf      = torch.tensor(t,        dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    logk_tf   = torch.tensor(logk,     dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    u_tf      = torch.tensor(u,        dtype=torch.float32, device=dev)                       # <<< CHANGED >>>\n",
    "    x_f_tf    = torch.tensor(x_f,      dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    t_f_tf    = torch.tensor(t_f,      dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    logk_f_tf = torch.tensor(logk_f,   dtype=torch.float32, device=dev, requires_grad=True)   # <<< CHANGED >>>\n",
    "    x_v_tf    = torch.tensor(x_v,      dtype=torch.float32, device=dev)\n",
    "    t_v_tf    = torch.tensor(t_v,      dtype=torch.float32, device=dev)\n",
    "    logk_v_tf = torch.tensor(logk_v,   dtype=torch.float32, device=dev)\n",
    "    u_true_tf = torch.tensor(u_v_true, dtype=torch.float32, device=dev).reshape(-1, 1)\n",
    "\n",
    "    mse_v_hist  = []\n",
    "    loss_values = []\n",
    "    max_errors  = []   # <<< NEW: to store (epoch, max_abs_error) >>>\n",
    "\n",
    "    patience  = 10000          # number of validations without improvement\n",
    "    pat       = 0\n",
    "    best_v    = float('inf')   # best validation MSE\n",
    "    best_TL   = float('inf')   # best training loss corresponding to best_v\n",
    "    best_max_err = float('inf')  # <<< NEW: best max |error| on val\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_ep    = -1\n",
    "\n",
    "    start_time  = time.time()\n",
    "    total_start = time.time()        # total wall-clock timer\n",
    "\n",
    "    for ep in range(nEpoch):\n",
    "\n",
    "        # ----- Stop if we've done lbfgs_epochs of L-BFGS -----\n",
    "        if used_lbfgs and lbfgs_start_ep is not None:\n",
    "            if ep - lbfgs_start_ep >= lbfgs_epochs:\n",
    "                print(f\"Stopping after {lbfgs_epochs} LBFGS epochs at global epoch {ep}\")\n",
    "                break\n",
    "\n",
    "        # ------------------------------\n",
    "        # STAGE 1: Adam (ep < switch_epoch)\n",
    "        # STAGE 2: L-BFGS (ep >= switch_epoch)\n",
    "        # ------------------------------\n",
    "        if ep < switch_epoch:\n",
    "            # ----- Adam update -----\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute predictions for training data (u)\n",
    "            u_pred = net_u(x_tf, t_tf, logk_tf, model)          # <<< CHANGED\n",
    "            # Compute PDE residual at collocation points\n",
    "            u_f_pred = net_f(x_f_tf, t_f_tf, logk_f_tf, model)  # <<< CHANGED\n",
    "    \n",
    "            loss_PDE  = criterion(u_f_pred, torch.zeros_like(u_f_pred))\n",
    "            loss_data = criterion(u_tf, u_pred)\n",
    "            loss = loss_PDE + 100 * loss_data\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        else:\n",
    "            # ----- Switch to L-BFGS once -----\n",
    "            if not used_lbfgs:\n",
    "                # 1) Load the best Adam weights BEFORE creating LBFGS\n",
    "                model.load_state_dict(best_state)\n",
    "        \n",
    "                # 2) Print which Adam state you're starting from\n",
    "                print(\n",
    "                    f\"Switching to L-BFGS at epoch {ep} \"\n",
    "                    f\"-> starting from Adam best at epoch {best_ep} \"\n",
    "                    f\"(TrainLoss={best_TL:.3e}, Val MSE={best_v:.3e})\"\n",
    "                )\n",
    "        \n",
    "                # 3) Create the LBFGS optimiser on top of that state\n",
    "                optimizer = torch.optim.LBFGS(\n",
    "                    model.parameters(),\n",
    "                    max_iter=20,          # internal LBFGS iterations per .step()\n",
    "                    history_size=100,\n",
    "                    line_search_fn=None\n",
    "                )\n",
    "                used_lbfgs = True\n",
    "                lbfgs_start_ep = ep      # <-- remember when we switched\n",
    "                \n",
    "\n",
    "            # L-BFGS requires a closure that re-computes the loss\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                u_pred   = net_u(x_tf, t_tf, logk_tf, model)          # <<< CHANGED\n",
    "                u_f_pred = net_f(x_f_tf, t_f_tf, logk_f_tf, model)    # <<< CHANGED\n",
    "\n",
    "                loss_PDE  = criterion(u_f_pred, torch.zeros_like(u_f_pred))\n",
    "                loss_data = criterion(u_tf, u_pred)\n",
    "                loss      = loss_PDE + 100 * loss_data\n",
    "\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            loss = optimizer.step(closure)  # returns the loss from the last closure call\n",
    "\n",
    "        \n",
    "        # ----- validation -----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            u_v_pred = net_u(x_v_tf, t_v_tf, logk_v_tf, model)   # <<< CHANGED\n",
    "            mse_v = criterion(u_v_pred, u_true_tf).item()\n",
    "            mse_v_hist.append((ep, mse_v))\n",
    "\n",
    "            # <<< NEW: max absolute error on validation set >>>\n",
    "            abs_err = torch.abs(u_v_pred - u_true_tf)   # (N_val, 1)\n",
    "            max_err = abs_err.max().item()              # scalar\n",
    "            max_errors.append((ep, max_err))            # store (epoch, max_err)\n",
    "            # -----------------------------------------------\n",
    "\n",
    "        model.train()  # switch back\n",
    "\n",
    "\n",
    "        # ----- early stopping on val -----\n",
    "        # if mse_v < best_v:\n",
    "        if loss.item() < best_TL:\n",
    "            best_v       = mse_v\n",
    "            best_TL      = loss.item()\n",
    "            best_max_err = max_err          # <<< NEW: store max error at best state\n",
    "            best_state   = copy.deepcopy(model.state_dict())\n",
    "            best_ep      = ep\n",
    "            # print(f\"[Improved] Epoch {ep} | Best Val MSE: {best_v:.3e}\")\n",
    "            pat = 0\n",
    "        else:\n",
    "            pat += 1\n",
    "            if pat >= patience:\n",
    "                print(f\"Early stopping at it={ep}, best Val MSE={best_v:.3e}\")\n",
    "                break\n",
    "        \n",
    "        # Print progress\n",
    "        # - Before LBFGS: every 1000 epochs\n",
    "        # - After LBFGS is enabled: every 100 epochs\n",
    "        if (not used_lbfgs and ep % 1000 == 0) or (used_lbfgs and ep % 1000 == 0):\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Epochs: {ep:6d} | TrainLoss: {loss.item():.3e} \"\n",
    "                  f\"| Val MSE: {mse_v:.3e} \"\n",
    "                  f\"| Max Val |err|: {max_err:.3e} \"   # <<< NEW\n",
    "                  f\"| Time: {elapsed:.2f}s\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "    total_elapsed = time.time() - total_start\n",
    "    print(f\"Total training time: {total_elapsed:.2f} s\")\n",
    "    print(f\"Best Val MSE: {best_v:.3e} at epoch {best_ep}\")\n",
    "    print(f\"Best Max |err| on validation: {best_max_err:.3e}\")   # <<< NEW\n",
    "\n",
    "    model.load_state_dict(best_state)          # <- load best here\n",
    "\n",
    "    return loss_values, mse_v_hist, max_errors, best_ep, best_TL, best_v, best_max_err\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Build dataset for arbitrary (N_i, N_b, N_k, N_f)\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_dataset(N_i, N_b, N_k, N_f, N_val,\n",
    "                  x_min, x_max,\n",
    "                  t_min, t_max,\n",
    "                  k_min, k_max,\n",
    "                  seed):\n",
    "    \"\"\"\n",
    "    Build training and collocation sets for the parametric heat equation.\n",
    "\n",
    "    Returns:\n",
    "        X_u_train : (N_u, 3) array of data points (x, t, log10(k)) for IC + BC.\n",
    "        u_train   : (N_u, 1) IC + BC at X_u_train.\n",
    "        X_f_train : (N_f, 3) collocation points in (x, t, log10(k)).\n",
    "        X_val     : (N_val, 3) validation points in (x, t, log10(k)).\n",
    "        lb, ub    : lower/upper bounds for normalisation in the NN.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- k and logk ---\n",
    "    # We sample k in [k_min, k_max] but represent it via log10(k).\n",
    "    logk_min = np.log10(k_min)\n",
    "    logk_max = np.log10(k_max)\n",
    "    logk_vec = np.linspace(logk_min, logk_max, N_k)  # equally spaced in log10(k)\n",
    "    k_vec    = 10.0**logk_vec                        # corresponding physical k\n",
    "\n",
    "    # --- Initial condition: u(x,0;k) = sin(pi x) ---\n",
    "    # x in [x_min, x_max], t=0, and all logk samples\n",
    "    x_ic = np.linspace(x_min, x_max, N_i)\n",
    "    t_ic = np.array([t_min])   # or [t_min]\n",
    "    x_ic_g, t_ic_g, logk_ic_g = np.meshgrid(x_ic, t_ic, logk_vec, indexing='ij')\n",
    "\n",
    "    x_u_ic    = x_ic_g.ravel()[:, None]\n",
    "    t_u_ic    = t_ic_g.ravel()[:, None]\n",
    "    logk_u_ic = logk_ic_g.ravel()[:, None]\n",
    "    X_u_train_ic = np.hstack([x_u_ic, t_u_ic, logk_u_ic])\n",
    "\n",
    "    # --- Boundary conditions: u(0,t;k)=0, u(1,t;k)=0 ---\n",
    "    # We discretise t with 2*N_b points between t_min and t_max.\n",
    "    t_line = np.linspace(t_min, t_max, 2 * N_b)\n",
    "    x_bc_left  = np.array([x_min])\n",
    "    x_bc_right = np.array([x_max])\n",
    "    x_bc = np.concatenate([x_bc_left, x_bc_right], axis=0)  # [0, 1]\n",
    "\n",
    "    x_bc_g, t_bc_g, logk_bc_g = np.meshgrid(x_bc, t_line, logk_vec, indexing='ij')\n",
    "    x_u_bc    = x_bc_g.ravel()[:, None]\n",
    "    t_u_bc    = t_bc_g.ravel()[:, None]\n",
    "    logk_u_bc = logk_bc_g.ravel()[:, None]\n",
    "    X_u_train_bc = np.hstack([x_u_bc, t_u_bc, logk_u_bc])\n",
    "\n",
    "    # --- Combine IC and BC into one \"data\" set ---\n",
    "    X_u_train = np.vstack([X_u_train_ic, X_u_train_bc]).astype(np.float32)\n",
    "\n",
    "    # --- Analytic solution for those IC/BC points ---\n",
    "    x_cal    = X_u_train[:, 0]\n",
    "    t_cal    = X_u_train[:, 1]\n",
    "    logk_cal = X_u_train[:, 2]\n",
    "    k_cal    = 10.0**logk_cal\n",
    "\n",
    "    # Closed-form solution: u(x,t;k) = exp(-k π² t) sin(π x)\n",
    "    u_train = np.exp(-k_cal * (np.pi**2) * t_cal) * np.sin(np.pi * x_cal)\n",
    "    u_train = u_train[:, None].astype(np.float32)\n",
    "\n",
    "    # --- Collocation + validation via LHS in (x, t, logk) ---\n",
    "    lb = np.array([x_min, t_min, logk_min], dtype=np.float32)\n",
    "    ub = np.array([x_max, t_max, logk_max], dtype=np.float32)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    U_all = lhs(3, samples=N_f + N_val)   # Latin Hypercube in [0,1]^3\n",
    "    X_all = lb + (ub - lb) * U_all        # map to [lb, ub] in (x, t, logk)\n",
    "    X_f_train = X_all[:N_f]\n",
    "    X_val     = X_all[N_f:]\n",
    "\n",
    "    return X_u_train, u_train, X_f_train, X_val, lb, ub\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Compute global relative L2 error for a trained model at a fixed k\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_rel_L2(model,\n",
    "                   x_min, x_max,\n",
    "                   t_min, t_max,\n",
    "                   k_val,\n",
    "                   Nx=100, Nt=100,\n",
    "                   device=device):\n",
    "    \"\"\"\n",
    "    Compute global relative L2 error of the model solution against the analytic\n",
    "    solution on a regular (x,t) grid at a fixed physical k = k_val.\n",
    "\n",
    "    rel_L2 = ||u_pred - u_true||_2 / ||u_true||_2\n",
    "    \"\"\"\n",
    "\n",
    "    # Build regular grid in x, t\n",
    "    x_test = np.linspace(x_min, x_max, Nx)\n",
    "    t_test = np.linspace(t_min, t_max, Nt)\n",
    "    logk_val = np.log10(k_val)  # convert to log10(k) for NN input\n",
    "\n",
    "    T, X = np.meshgrid(t_test, x_test, indexing='ij')  # shape (Nt, Nx)\n",
    "    LOGK = np.full_like(T, logk_val)                   # broadcast log10(k_val)\n",
    "\n",
    "    # Flatten to (Nt*Nx, 1) column vectors\n",
    "    x_flat    = X.ravel()[:, None]\n",
    "    t_flat    = T.ravel()[:, None]\n",
    "    logk_flat = LOGK.ravel()[:, None]\n",
    "\n",
    "    # Stack into (Nt*Nx, 3) array and convert to torch tensor\n",
    "    X_star    = np.hstack([x_flat, t_flat, logk_flat]).astype(np.float32)\n",
    "    X_star_tf = torch.from_numpy(X_star).to(device)\n",
    "\n",
    "    # NN prediction over the grid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        u_pred = model(X_star_tf).squeeze(1).cpu().numpy().reshape(T.shape)\n",
    "\n",
    "    # Analytic solution on the same grid\n",
    "    u_true = u_true_numpy(X, T, LOGK)\n",
    "\n",
    "    # Global relative L2 error\n",
    "    num = np.linalg.norm(u_pred - u_true)\n",
    "    den = np.linalg.norm(u_true)\n",
    "    rel_L2 = num / den if den > 0 else num\n",
    "\n",
    "    return rel_L2\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Plot training curves (loss, val MSE, max |error|) and save to file\n",
    "# -----------------------------------------------------------------------------\n",
    "def plot_training_curves(loss_values, mse_v_hist, max_errors,\n",
    "                         N_i, N_b, N_k, N_f,\n",
    "                         out_dir=\"sweep_results\"):\n",
    "    \"\"\"\n",
    "    Make the TrainLoss / ValMSE / Max|Error| vs epoch plot and save to file.\n",
    "\n",
    "    Args:\n",
    "        loss_values : list of train loss per epoch\n",
    "        mse_v_hist  : list of (epoch, val_MSE)\n",
    "        max_errors  : list of (epoch, max_abs_error) on validation\n",
    "        N_i, N_b, N_k, N_f : configuration used (for filename)\n",
    "        out_dir     : directory where the PNG is saved\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Training loss epochs\n",
    "    ep_train = range(len(loss_values))\n",
    "\n",
    "    # Validation MSE: unpack (epoch, mse)\n",
    "    ep_val  = [int(i) for i, _ in mse_v_hist]\n",
    "    mse_val = [\n",
    "        (m.detach().cpu().item() if torch.is_tensor(m) else float(m))\n",
    "        for _, m in mse_v_hist\n",
    "    ]\n",
    "\n",
    "    # Max absolute error: unpack (epoch, max_err)\n",
    "    ep_max   = [int(i) for i, _ in max_errors]\n",
    "    max_errs = [\n",
    "        (m.detach().cpu().item() if torch.is_tensor(m) else float(m))\n",
    "        for _, m in max_errors\n",
    "    ]\n",
    "\n",
    "    # Plot curves on log scale (since errors/loss typically span many orders)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(ep_train, loss_values, color='black', label='Train Loss')\n",
    "    plt.plot(ep_val,   mse_val,     color='red',   label='Validation MSE')\n",
    "    plt.plot(ep_max,   max_errs,    color='blue',  label='Max |Error| (Validation)')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss / Error')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Training Loss, Validation MSE, and Max Validation Error vs Iterations')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # File name tagged with the configuration\n",
    "    fname = f\"train_Ni{N_i}_Nb{N_b}_Nk{N_k}_Nf{N_f}.png\"\n",
    "    fpath = os.path.join(out_dir, fname)\n",
    "    plt.savefig(fpath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Run a single experiment for given (N_i, N_b, N_k, N_f)\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_single_experiment(N_i, N_b, N_k, N_f, seed,\n",
    "                          N_val,\n",
    "                          Train_epochs,\n",
    "                          learning_rate,\n",
    "                          k_val_eval,\n",
    "                          results_dir=\"sweep_results\"):\n",
    "    \"\"\"\n",
    "    Runs one full experiment:\n",
    "      - builds dataset for given (N_i, N_b, N_k, N_f)\n",
    "      - trains a fresh model\n",
    "      - saves training curve plot\n",
    "      - computes global rel L2 error at k = k_val_eval\n",
    "      - returns a dict with all requested info\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Build dataset (IC+BC data, collocation, validation, bounds)\n",
    "    X_u_train, u_train, X_f_train, X_val, lb, ub = build_dataset(\n",
    "        N_i=N_i, N_b=N_b, N_k=N_k, N_f=N_f, N_val=N_val,\n",
    "        x_min=x_min, x_max=x_max, t_min=t_min, t_max=t_max,\n",
    "        k_min=k_min, k_max=k_max, seed=seed\n",
    "    )\n",
    "\n",
    "    # 2) Initialise a new PINN model for this dataset\n",
    "    model = NeuralNet(layers, lb, ub).to(device).float()\n",
    "\n",
    "    # 3) Train and measure total wall-clock time for this configuration\n",
    "    exp_start = time.time()\n",
    "    loss_values, mse_v_hist, max_errors, best_ep, best_TL, best_v, best_max_err = train(\n",
    "        Train_epochs,\n",
    "        X_u_train,\n",
    "        u_train,\n",
    "        X_f_train,\n",
    "        X_val,\n",
    "        model,\n",
    "        learning_rate\n",
    "    )\n",
    "    total_elapsed = time.time() - exp_start\n",
    "\n",
    "    # 4) Compute global relative L2 error at a chosen k (e.g. k = 1.0)\n",
    "    rel_L2 = compute_rel_L2(\n",
    "        model,\n",
    "        x_min=x_min, x_max=x_max,\n",
    "        t_min=t_min, t_max=t_max,\n",
    "        k_val=k_val_eval,\n",
    "        Nx=100, Nt=100,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 5) Create and save the training curve plot for this run\n",
    "    curve_path = plot_training_curves(\n",
    "        loss_values, mse_v_hist, max_errors,\n",
    "        N_i=N_i, N_b=N_b, N_k=N_k, N_f=N_f,\n",
    "        out_dir=results_dir\n",
    "    )\n",
    "\n",
    "    # 6) Pack all information into a record dictionary\n",
    "    record = {\n",
    "        \"N_i\": N_i,\n",
    "        \"N_b\": N_b,\n",
    "        \"N_k\": N_k,\n",
    "        \"N_f\": N_f,\n",
    "        \"total_elapsed\": total_elapsed,\n",
    "        \"best_ep\": best_ep,\n",
    "        \"best_TL\": best_TL,\n",
    "        \"best_v\": best_v,\n",
    "        \"best_max_err\": best_max_err,\n",
    "        \"rel_L2\": rel_L2,\n",
    "        \"loss_values\": loss_values,\n",
    "        \"mse_v_hist\": mse_v_hist,\n",
    "        \"max_errors\": max_errors,\n",
    "        \"training_curve_path\": curve_path,\n",
    "    }\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Sweep settings and loops for N_f, N_i, N_b, N_k\n",
    "# -----------------------------------------------------------------------------\n",
    "layers = [3, 50, 50, 50, 1]\n",
    "x_min=0.0\n",
    "x_max=1.0\n",
    "t_min=0.0\n",
    "t_max=0.25\n",
    "k_min=0.2\n",
    "k_max=2.0\n",
    "seed=123\n",
    "    \n",
    "# Base values (same as your current defaults)\n",
    "BASE_N_i = 101\n",
    "BASE_N_b = 51\n",
    "BASE_N_k = 51\n",
    "BASE_N_f = 1000\n",
    "\n",
    "# Training hyperparameters for all sweeps\n",
    "Train_epochs = 100000\n",
    "learning_rate = 0.0005\n",
    "N_val = 100          # number of validation points from LHS\n",
    "k_val_eval = 1.0     # k at which global rel L2 is computed\n",
    "\n",
    "# ==========================\n",
    "# Sweep 1: Collocation points N_f\n",
    "# ==========================\n",
    "Nf_list = [1000, 10000, 50000, 100000]  # values to test for N_f\n",
    "\n",
    "results_Nf = []\n",
    "\n",
    "for N_f in Nf_list:\n",
    "    print(f\"\\n=== Sweep N_f = {N_f} (N_i={BASE_N_i}, N_b={BASE_N_b}, N_k={BASE_N_k}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=BASE_N_i,\n",
    "        N_b=BASE_N_b,\n",
    "        N_k=BASE_N_k,\n",
    "        N_f=N_f,\n",
    "        seed=123,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Nf\",\n",
    "    )\n",
    "    results_Nf.append(rec)\n",
    "\n",
    "# Convert list of records to DataFrame for easy inspection and saving\n",
    "df_Nf = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        # Histories stored as objects (lists) – still useful inside Python\n",
    "        # \"loss_values\": r[\"loss_values\"],\n",
    "        # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        # \"max_errors\": r[\"max_errors\"],\n",
    "        # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Nf\n",
    "])\n",
    "\n",
    "print(\"\\nN_f sweep summary:\")\n",
    "display(df_Nf)\n",
    "\n",
    "# Save N_f sweep summary table as Excel\n",
    "df_Nf.to_excel(\"sweep_Nf_summary.xlsx\", index=False)\n",
    "\n",
    "# ==========================\n",
    "# Sweep 2: IC points N_i\n",
    "# ==========================\n",
    "Ni_list = [101, 1001, 5001, 10001]  # values to test for N_i\n",
    "\n",
    "results_Ni = []\n",
    "\n",
    "for N_i in Ni_list:\n",
    "    print(f\"\\n=== Sweep N_i = {N_i} (N_b={BASE_N_b}, N_k={BASE_N_k}, N_f={BASE_N_f}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=N_i,\n",
    "        N_b=BASE_N_b,\n",
    "        N_k=BASE_N_k,\n",
    "        N_f=BASE_N_f,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Ni\",\n",
    "        seed=123\n",
    "    )\n",
    "    results_Ni.append(rec)\n",
    "\n",
    "df_Ni = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        # \"loss_values\": r[\"loss_values\"],\n",
    "        # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        # \"max_errors\": r[\"max_errors\"],\n",
    "        # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Ni\n",
    "])\n",
    "\n",
    "print(\"\\nN_i sweep summary:\")\n",
    "display(df_Ni)\n",
    "\n",
    "df_Ni.to_excel(\"sweep_Ni_summary.xlsx\", index=False)\n",
    "\n",
    "# ==========================\n",
    "# Sweep 3: BC points N_b\n",
    "# ==========================\n",
    "Nb_list = [51, 501, 2501, 5001]  # values to test for N_b\n",
    "\n",
    "results_Nb = []\n",
    "\n",
    "for N_b in Nb_list:\n",
    "    print(f\"\\n=== Sweep N_b = {N_b} (N_i={BASE_N_i}, N_k={BASE_N_k}, N_f={BASE_N_f}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=BASE_N_i,\n",
    "        N_b=N_b,\n",
    "        N_k=BASE_N_k,\n",
    "        N_f=BASE_N_f,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Nb\",\n",
    "        seed=123\n",
    "    )\n",
    "    results_Nb.append(rec)\n",
    "\n",
    "df_Nb = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        # \"loss_values\": r[\"loss_values\"],\n",
    "        # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        # \"max_errors\": r[\"max_errors\"],\n",
    "        # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Nb\n",
    "])\n",
    "\n",
    "print(\"\\nN_b sweep summary:\")\n",
    "display(df_Nb)\n",
    "\n",
    "df_Nb.to_excel(\"sweep_Nb_summary.xlsx\", index=False)\n",
    "\n",
    "# ==========================\n",
    "# Sweep 4: parameter samples N_k\n",
    "# ==========================\n",
    "Nk_list = [51, 501, 2501, 5001]  # values to test for N_k\n",
    "\n",
    "results_Nk = []\n",
    "\n",
    "for N_k in Nk_list:\n",
    "    print(f\"\\n=== Sweep N_k = {N_k} (N_i={BASE_N_i}, N_b={BASE_N_b}, N_f={BASE_N_f}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=BASE_N_i,\n",
    "        N_b=BASE_N_b,\n",
    "        N_k=N_k,\n",
    "        N_f=BASE_N_f,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Nk\",\n",
    "        seed=123\n",
    "    )\n",
    "    results_Nk.append(rec)\n",
    "\n",
    "df_Nk = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        # \"loss_values\": r[\"loss_values\"],\n",
    "        # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        # \"max_errors\": r[\"max_errors\"],\n",
    "        # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Nk\n",
    "])\n",
    "\n",
    "print(\"\\nN_k sweep summary:\")\n",
    "display(df_Nk)\n",
    "\n",
    "df_Nk.to_excel(\"sweep_Nk_summary.xlsx\", index=False)\n",
    "\n",
    "print(\"\\n=== END OF SCRIPT ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758839791644,
     "user": {
      "displayName": "Oscar Zhang",
      "userId": "08884929965206842440"
     },
     "user_tz": -60
    },
    "id": "jQWrqMPo5Apy"
   },
   "outputs": [],
   "source": [
    "# t = data['t'].flatten()[:,None] # read in t and flatten into column vector\n",
    "# x = data['x'].flatten()[:,None] # read in x and flatten into column vector\n",
    "#  # Exact represents the exact solution to the problem, from the data provided\n",
    "# Exact = np.real(data['usol']).T # Exact has structure of nt times nx\n",
    "\n",
    "# print(\"usol shape (nt, nx) = \", Exact.shape)\n",
    "\n",
    "# # We need to find all the x,t coordinate pairs in the domain\n",
    "# X, T = np.meshgrid(x,t)\n",
    "\n",
    "# # Flatten the coordinate grid into pairs of x,t coordinates\n",
    "# X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # coordinates x,t\n",
    "# u_star = Exact.flatten()[:,None]   # corresponding solution value with each coordinate\n",
    "\n",
    "\n",
    "# print(\"X has shape \", X.shape, \", X_star has shape \", X_star.shape, \", u_star has shape \", u_star.shape)\n",
    "\n",
    "# # Domain bounds (-1,1)\n",
    "# lb = X_star.min(axis=0)\n",
    "# ub = X_star.max(axis=0)\n",
    "\n",
    "# print(\"Lower bounds of x,t: \", lb)\n",
    "# print(\"Upper bounds of x,t: \", ub)\n",
    "# print('')\n",
    "# print('The first few entries of X_star are:')\n",
    "# print( X_star[0:5, :] )\n",
    "\n",
    "# print('')\n",
    "# print('The first few entries of u_star are:')\n",
    "# print( u_star[0:5, :] )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

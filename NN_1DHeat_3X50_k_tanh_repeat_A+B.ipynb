{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1758839791622,
     "user": {
      "displayName": "Oscar Zhang",
      "userId": "08884929965206842440"
     },
     "user_tz": -60
    },
    "id": "fWGYfkhF4JUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? True\n",
      "Device: NVIDIA A2\n",
      "Using device: cuda\n",
      "\n",
      "=== Repeat 1/5 with N_f = 1000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 2.018e+01 | Val MSE: 5.257e+00 | Max Val |err|: 1.018e+01 | Time: 0.23s\n",
      "Epochs:   1000 | TrainLoss: 3.442e+00 | Val MSE: 3.219e+00 | Max Val |err|: 8.426e+00 | Time: 3.41s\n",
      "Epochs:   2000 | TrainLoss: 6.850e-01 | Val MSE: 4.797e-01 | Max Val |err|: 3.377e+00 | Time: 3.42s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2986 (TrainLoss=1.133e-01, Val MSE=3.646e-02)\n",
      "Epochs:   3000 | TrainLoss: 1.133e-01 | Val MSE: 1.107e-02 | Max Val |err|: 4.899e-01 | Time: 3.39s\n",
      "Epochs:   4000 | TrainLoss: 2.995e-05 | Val MSE: 3.598e-05 | Max Val |err|: 3.627e-02 | Time: 87.84s\n",
      "Epochs:   5000 | TrainLoss: 2.995e-05 | Val MSE: 3.598e-05 | Max Val |err|: 3.627e-02 | Time: 8.67s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 115.42 s\n",
      "Best Val MSE: 3.598e-05 at epoch 3607\n",
      "Best Max |err| on validation: 3.627e-02\n",
      "\n",
      "=== Repeat 2/5 with N_f = 1000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 1.394e+01 | Val MSE: 4.757e+00 | Max Val |err|: 9.833e+00 | Time: 0.01s\n",
      "Epochs:   1000 | TrainLoss: 3.715e+00 | Val MSE: 3.373e+00 | Max Val |err|: 8.620e+00 | Time: 3.43s\n",
      "Epochs:   2000 | TrainLoss: 1.148e+00 | Val MSE: 8.780e-01 | Max Val |err|: 4.193e+00 | Time: 3.47s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=3.452e-01, Val MSE=1.680e-01)\n",
      "Epochs:   3000 | TrainLoss: 3.449e-01 | Val MSE: 5.773e-02 | Max Val |err|: 1.088e+00 | Time: 3.51s\n",
      "Epochs:   4000 | TrainLoss: 3.361e-05 | Val MSE: 1.507e-05 | Max Val |err|: 2.431e-02 | Time: 114.32s\n",
      "Epochs:   5000 | TrainLoss: 3.361e-05 | Val MSE: 1.507e-05 | Max Val |err|: 2.431e-02 | Time: 9.68s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 143.46 s\n",
      "Best Val MSE: 1.507e-05 at epoch 3895\n",
      "Best Max |err| on validation: 2.431e-02\n",
      "\n",
      "=== Repeat 3/5 with N_f = 1000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 2.870e+01 | Val MSE: 5.001e+00 | Max Val |err|: 9.988e+00 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 4.704e+00 | Val MSE: 3.954e+00 | Max Val |err|: 9.269e+00 | Time: 3.47s\n",
      "Epochs:   2000 | TrainLoss: 2.383e+00 | Val MSE: 2.154e+00 | Max Val |err|: 7.004e+00 | Time: 3.51s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=3.691e-01, Val MSE=1.976e-01)\n",
      "Epochs:   3000 | TrainLoss: 3.687e-01 | Val MSE: 5.337e-02 | Max Val |err|: 1.134e+00 | Time: 3.41s\n",
      "Epochs:   4000 | TrainLoss: 2.563e-05 | Val MSE: 4.982e-05 | Max Val |err|: 5.173e-02 | Time: 107.78s\n",
      "Epochs:   5000 | TrainLoss: 2.563e-05 | Val MSE: 4.982e-05 | Max Val |err|: 5.173e-02 | Time: 8.44s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 135.14 s\n",
      "Best Val MSE: 4.982e-05 at epoch 3868\n",
      "Best Max |err| on validation: 5.173e-02\n",
      "\n",
      "=== Repeat 4/5 with N_f = 1000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 3.175e+01 | Val MSE: 4.385e+00 | Max Val |err|: 9.533e+00 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 4.462e+00 | Val MSE: 3.931e+00 | Max Val |err|: 9.292e+00 | Time: 3.39s\n",
      "Epochs:   2000 | TrainLoss: 2.177e+00 | Val MSE: 1.927e+00 | Max Val |err|: 6.630e+00 | Time: 3.43s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2995 (TrainLoss=3.653e-01, Val MSE=2.038e-01)\n",
      "Epochs:   3000 | TrainLoss: 3.683e-01 | Val MSE: 5.919e-02 | Max Val |err|: 1.153e+00 | Time: 3.45s\n",
      "Epochs:   4000 | TrainLoss: 3.131e-05 | Val MSE: 4.420e-05 | Max Val |err|: 3.375e-02 | Time: 121.51s\n",
      "Epochs:   5000 | TrainLoss: 2.452e-05 | Val MSE: 4.911e-05 | Max Val |err|: 3.802e-02 | Time: 25.83s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 166.22 s\n",
      "Best Val MSE: 4.911e-05 at epoch 4341\n",
      "Best Max |err| on validation: 3.802e-02\n",
      "\n",
      "=== Repeat 5/5 with N_f = 1000 (N_i=101, N_b=51, N_k=51) ===\n",
      "Epochs:      0 | TrainLoss: 2.040e+01 | Val MSE: 4.798e+00 | Max Val |err|: 9.833e+00 | Time: 0.00s\n",
      "Epochs:   1000 | TrainLoss: 4.240e+00 | Val MSE: 3.690e+00 | Max Val |err|: 8.991e+00 | Time: 3.58s\n",
      "Epochs:   2000 | TrainLoss: 1.037e+00 | Val MSE: 7.973e-01 | Max Val |err|: 4.316e+00 | Time: 3.57s\n",
      "Switching to L-BFGS at epoch 3000 -> starting from Adam best at epoch 2999 (TrainLoss=1.497e-01, Val MSE=5.292e-02)\n",
      "Epochs:   3000 | TrainLoss: 1.495e-01 | Val MSE: 1.082e-02 | Max Val |err|: 5.457e-01 | Time: 3.50s\n",
      "Epochs:   4000 | TrainLoss: 2.158e-05 | Val MSE: 1.313e-05 | Max Val |err|: 2.395e-02 | Time: 87.16s\n",
      "Epochs:   5000 | TrainLoss: 2.158e-05 | Val MSE: 1.313e-05 | Max Val |err|: 2.395e-02 | Time: 8.28s\n",
      "Stopping after 3000.0 LBFGS epochs at global epoch 6000\n",
      "Total training time: 114.36 s\n",
      "Best Val MSE: 1.313e-05 at epoch 3656\n",
      "Best Max |err| on validation: 2.395e-02\n",
      "\n",
      "Repeated runs summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repeat</th>\n",
       "      <th>N_i</th>\n",
       "      <th>N_b</th>\n",
       "      <th>N_k</th>\n",
       "      <th>N_f</th>\n",
       "      <th>total_elapsed</th>\n",
       "      <th>best_ep</th>\n",
       "      <th>best_TL</th>\n",
       "      <th>best_v</th>\n",
       "      <th>best_max_err</th>\n",
       "      <th>rel_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.440816</td>\n",
       "      <td>3607</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.036273</td>\n",
       "      <td>0.000835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>143.462456</td>\n",
       "      <td>3895</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.024312</td>\n",
       "      <td>0.000670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>135.137108</td>\n",
       "      <td>3868</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.051735</td>\n",
       "      <td>0.001172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>166.219461</td>\n",
       "      <td>4341</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.038024</td>\n",
       "      <td>0.001209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.364171</td>\n",
       "      <td>3656</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.023948</td>\n",
       "      <td>0.000545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   repeat  N_i  N_b  N_k   N_f  total_elapsed  best_ep   best_TL    best_v  \\\n",
       "0       1  101   51   51  1000     116.440816     3607  0.000030  0.000036   \n",
       "1       2  101   51   51  1000     143.462456     3895  0.000034  0.000015   \n",
       "2       3  101   51   51  1000     135.137108     3868  0.000026  0.000050   \n",
       "3       4  101   51   51  1000     166.219461     4341  0.000025  0.000049   \n",
       "4       5  101   51   51  1000     114.364171     3656  0.000022  0.000013   \n",
       "\n",
       "   best_max_err    rel_L2  \n",
       "0      0.036273  0.000835  \n",
       "1      0.024312  0.000670  \n",
       "2      0.051735  0.001172  \n",
       "3      0.038024  0.001209  \n",
       "4      0.023948  0.000545  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== END OF SCRIPT ===\n"
     ]
    }
   ],
   "source": [
    "# For readability: disable warnings from libraries like matplotlib, etc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "# Make sure torch is imported somewhere above this cell:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "from itertools import product, combinations\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as la\n",
    "from pyDOE import lhs\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogLocator, FuncFormatter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# --- Device Setup ---\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Select the most performant device available (CUDA > MPS > CPU)\n",
    "device = (\n",
    "    torch.device('cuda') if torch.cuda.is_available()\n",
    "    else torch.device('mps') if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else torch.device('cpu')\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "def u_true_numpy(X, T, K):\n",
    "    \"\"\"Vectorised true solution: U = exp(-(pi^2) * T) * sin(pi * X).\"\"\"\n",
    "    D = np.tanh(K)\n",
    "    return np.exp(- D * (np.pi**2) * T) * np.sin(np.pi * X)\n",
    "\n",
    "def net_u(x, t, k, model):\n",
    "    \"\"\"\n",
    "    NN input is (x, t, l).\n",
    "    \"\"\"\n",
    "    X = torch.cat([x, t, k], dim=1)  # If x and t are each shape (N, 1), then X becomes (N, 2).\n",
    "    u = model(X)\n",
    "    return u\n",
    "\n",
    "# net_f computes the PDE residual\n",
    "# If f ≈ 0 at collocation points, the NN satisfies the equation there\n",
    "def net_f(x, t, k, model):\n",
    "    x.requires_grad_(True)\n",
    "    t.requires_grad_(True)\n",
    "    # k.requires_grad_(True)\n",
    "    \n",
    "    u = net_u(x, t, k, model)\n",
    "    \n",
    "    u_x = torch.autograd.grad(\n",
    "        u, x, grad_outputs=torch.ones_like(u),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    u_t = torch.autograd.grad(\n",
    "        u, t, grad_outputs=torch.ones_like(u),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    u_xx = torch.autograd.grad(\n",
    "        u_x, x, grad_outputs=torch.ones_like(u_x),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    D = torch.tanh(k)       # diffusivity = tanh(k)\n",
    "    f = u_t - D * u_xx      # u_t = tanh(k) * u_xx\n",
    "    return f\n",
    "\n",
    "class XavierInit(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(XavierInit, self).__init__()\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = torch.sqrt(torch.tensor(2.0 / (in_dim + out_dim)))\n",
    "        self.weight = nn.Parameter(torch.randn(in_dim, out_dim) * xavier_stddev)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "def initialize_NN(layers):\n",
    "    weights = nn.ModuleList()\n",
    "    num_layers = len(layers)\n",
    "    for l in range(num_layers - 1):\n",
    "        layer = XavierInit(size=[layers[l], layers[l + 1]]) # if there was no retutn, how do I get the weight and bias?\n",
    "        weights.append(layer)\n",
    "    return weights\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, layers, lb, ub):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.weights = initialize_NN(layers)\n",
    "        # make lb/ub move with .to(device)\n",
    "        self.register_buffer('lb', torch.as_tensor(lb, dtype=torch.float32))     # <<< CHANGED >>>\n",
    "        self.register_buffer('ub', torch.as_tensor(ub, dtype=torch.float32))     # <<< CHANGED >>>\n",
    "        # self.register_buffer('k', torch.tensor(k_init, dtype=torch.float32))     # <<< CHANGED >>>\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.float()                                                            # <<< CHANGED >>>\n",
    "        lb = self.lb.to(X.device)                                                # <<< CHANGED >>>\n",
    "        ub = self.ub.to(X.device)                                                # <<< CHANGED >>>\n",
    "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        for l in range(len(self.weights) - 1):\n",
    "            H = torch.tanh(self.weights[l](H.float()))     # Is this already a calculation?\n",
    "        Y = self.weights[-1](H)\n",
    "        return Y\n",
    "\n",
    "def train(nEpoch, X, u, X_f, X_val, model, learning_rate):\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # ----- STAGE 1: start with Adam -----\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # when to switch from Adam to L-BFGS\n",
    "    switch_epoch = 3000.0\n",
    "    used_lbfgs   = False   \n",
    "    lbfgs_epochs   = 3000.0           # <-- how many epochs of L-BFGS you want\n",
    "    lbfgs_start_ep = None             # <-- will store the epoch where we switch\n",
    "\n",
    "    # use the model's device\n",
    "    dev = next(model.parameters()).device                                        # <<< CHANGED >>>\n",
    "\n",
    "    x    = X[:, 0:1]\n",
    "    t    = X[:, 1:2]\n",
    "    k    = X[:, 2:3]\n",
    "    # Collocation points (f points)\n",
    "    x_f    = X_f[:, 0:1]\n",
    "    t_f    = X_f[:, 1:2]\n",
    "    k_f    = X_f[:, 2:3]\n",
    "    # Validation points\n",
    "    x_v    = X_val[:, 0:1]\n",
    "    t_v    = X_val[:, 1:2]\n",
    "    k_v    = X_val[:, 2:3]\n",
    "\n",
    "    # True validation solution (analytic)\n",
    "    u_v_true = u_true_numpy(x_v, t_v, k_v)   # shape (N_val,)\n",
    "\n",
    "    # ------------- create tensors ON THE SAME DEVICE -------------\n",
    "    x_tf   = torch.tensor(x,   dtype=torch.float32, device=dev, requires_grad=True)\n",
    "    t_tf   = torch.tensor(t,   dtype=torch.float32, device=dev, requires_grad=True)\n",
    "    k_tf   = torch.tensor(k,   dtype=torch.float32, device=dev, requires_grad=True)\n",
    "\n",
    "    u_tf   = torch.tensor(u,   dtype=torch.float32, device=dev)  # training data (no grad)\n",
    "\n",
    "    x_f_tf = torch.tensor(x_f, dtype=torch.float32, device=dev, requires_grad=True)\n",
    "    t_f_tf = torch.tensor(t_f, dtype=torch.float32, device=dev, requires_grad=True)\n",
    "    k_f_tf = torch.tensor(k_f, dtype=torch.float32, device=dev, requires_grad=True)\n",
    "\n",
    "    x_v_tf    = torch.tensor(x_v,      dtype=torch.float32, device=dev)\n",
    "    t_v_tf    = torch.tensor(t_v,      dtype=torch.float32, device=dev)\n",
    "    k_v_tf    = torch.tensor(k_v,      dtype=torch.float32, device=dev)\n",
    "    u_true_tf = torch.tensor(u_v_true, dtype=torch.float32, device=dev).reshape(-1, 1)\n",
    "\n",
    "    mse_v_hist  = []\n",
    "    loss_values = []\n",
    "    max_errors  = []   # <<< NEW: to store (epoch, max_abs_error) >>>\n",
    "\n",
    "    patience  = 10000          # number of validations without improvement\n",
    "    pat       = 0\n",
    "    best_v    = float('inf')   # best validation MSE\n",
    "    best_TL   = float('inf')   # best training loss corresponding to best_v\n",
    "    best_max_err = float('inf')  # <<< NEW: best max |error| on val\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_ep    = -1\n",
    "\n",
    "    start_time  = time.time()\n",
    "    total_start = time.time()        # total wall-clock timer\n",
    "\n",
    "    for ep in range(nEpoch):\n",
    "\n",
    "        # ----- Stop if we've done lbfgs_epochs of L-BFGS -----\n",
    "        if used_lbfgs and lbfgs_start_ep is not None:\n",
    "            if ep - lbfgs_start_ep >= lbfgs_epochs:\n",
    "                print(f\"Stopping after {lbfgs_epochs} LBFGS epochs at global epoch {ep}\")\n",
    "                break\n",
    "\n",
    "        # ------------------------------\n",
    "        # STAGE 1: Adam (ep < switch_epoch)\n",
    "        # STAGE 2: L-BFGS (ep >= switch_epoch)\n",
    "        # ------------------------------\n",
    "        if ep < switch_epoch:\n",
    "            # ----- Adam update -----\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute predictions for training data (u)\n",
    "            u_pred = net_u(x_tf, t_tf, k_tf, model)\n",
    "            # Compute PDE residual at collocation points\n",
    "            u_f_pred = net_f(x_f_tf, t_f_tf, k_f_tf, model)\n",
    "    \n",
    "            loss_PDE  = criterion(u_f_pred, torch.zeros_like(u_f_pred))\n",
    "            loss_data = criterion(u_tf, u_pred)\n",
    "            loss = loss_PDE + 100 * loss_data\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        else:\n",
    "            # ----- Switch to L-BFGS once -----\n",
    "            if not used_lbfgs:\n",
    "                # 1) Load the best Adam weights BEFORE creating LBFGS\n",
    "                model.load_state_dict(best_state)\n",
    "        \n",
    "                # 2) Print which Adam state you're starting from\n",
    "                print(\n",
    "                    f\"Switching to L-BFGS at epoch {ep} \"\n",
    "                    f\"-> starting from Adam best at epoch {best_ep} \"\n",
    "                    f\"(TrainLoss={best_TL:.3e}, Val MSE={best_v:.3e})\"\n",
    "                )\n",
    "        \n",
    "                # 3) Create the LBFGS optimiser on top of that state\n",
    "                optimizer = torch.optim.LBFGS(\n",
    "                    model.parameters(),\n",
    "                    max_iter=20,          # internal LBFGS iterations per .step()\n",
    "                    history_size=100,\n",
    "                    line_search_fn=None\n",
    "                )\n",
    "                used_lbfgs = True\n",
    "                lbfgs_start_ep = ep      # <-- remember when we switched\n",
    "                \n",
    "\n",
    "            # L-BFGS requires a closure that re-computes the loss\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                u_pred   = net_u(x_tf, t_tf, k_tf, model)          # <<< CHANGED\n",
    "                u_f_pred = net_f(x_f_tf, t_f_tf, k_f_tf, model)    # <<< CHANGED\n",
    "\n",
    "                loss_PDE  = criterion(u_f_pred, torch.zeros_like(u_f_pred))\n",
    "                loss_data = criterion(u_tf, u_pred)\n",
    "                loss      = loss_PDE + 100 * loss_data\n",
    "\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            loss = optimizer.step(closure)  # returns the loss from the last closure call\n",
    "\n",
    "        \n",
    "        # ----- validation -----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            u_v_pred = net_u(x_v_tf, t_v_tf, k_v_tf, model)   # <<< CHANGED\n",
    "            mse_v = criterion(u_v_pred, u_true_tf).item()\n",
    "            mse_v_hist.append((ep, mse_v))\n",
    "\n",
    "            # <<< NEW: max absolute error on validation set >>>\n",
    "            abs_err = torch.abs(u_v_pred - u_true_tf)   # (N_val, 1)\n",
    "            max_err = abs_err.max().item()              # scalar\n",
    "            max_errors.append((ep, max_err))            # store (epoch, max_err)\n",
    "            # -----------------------------------------------\n",
    "\n",
    "        model.train()  # switch back\n",
    "\n",
    "\n",
    "        # ----- early stopping on val -----\n",
    "        # if mse_v < best_v:\n",
    "        if loss.item() < best_TL:\n",
    "            best_v       = mse_v\n",
    "            best_TL      = loss.item()\n",
    "            best_max_err = max_err          # <<< NEW: store max error at best state\n",
    "            best_state   = copy.deepcopy(model.state_dict())\n",
    "            best_ep      = ep\n",
    "            # print(f\"[Improved] Epoch {ep} | Best Val MSE: {best_v:.3e}\")\n",
    "            pat = 0\n",
    "        else:\n",
    "            pat += 1\n",
    "            if pat >= patience:\n",
    "                print(f\"Early stopping at it={ep}, best Val MSE={best_v:.3e}\")\n",
    "                break\n",
    "        \n",
    "        # Print progress\n",
    "        # - Before LBFGS: every 1000 epochs\n",
    "        # - After LBFGS is enabled: every 100 epochs\n",
    "        if (not used_lbfgs and ep % 1000 == 0) or (used_lbfgs and ep % 1000 == 0):\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Epochs: {ep:6d} | TrainLoss: {loss.item():.3e} \"\n",
    "                  f\"| Val MSE: {mse_v:.3e} \"\n",
    "                  f\"| Max Val |err|: {max_err:.3e} \"   # <<< NEW\n",
    "                  f\"| Time: {elapsed:.2f}s\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "    total_elapsed = time.time() - total_start\n",
    "    print(f\"Total training time: {total_elapsed:.2f} s\")\n",
    "    print(f\"Best Val MSE: {best_v:.3e} at epoch {best_ep}\")\n",
    "    print(f\"Best Max |err| on validation: {best_max_err:.3e}\")   # <<< NEW\n",
    "\n",
    "    model.load_state_dict(best_state)          # <- load best here\n",
    "\n",
    "    return loss_values, mse_v_hist, max_errors, best_ep, best_TL, best_v, best_max_err\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Build dataset for arbitrary (N_i, N_b, N_k, N_f)\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_dataset(N_i, N_b, N_k, N_f, N_val,\n",
    "                  x_min, x_max,\n",
    "                  t_min, t_max,\n",
    "                  k_min, k_max,\n",
    "                  seed):\n",
    "    \"\"\"\n",
    "    Build training and collocation sets for the parametric heat equation.\n",
    "\n",
    "    Returns:\n",
    "        X_u_train : (N_u, 3) array of data points (x, t, k) for IC + BC.\n",
    "        u_train   : (N_u, 1) IC + BC at X_u_train.\n",
    "        X_f_train : (N_f, 3) collocation points in (x, t, k).\n",
    "        X_val     : (N_val, 3) validation points in (x, t, k).\n",
    "        lb, ub    : lower/upper bounds for normalisation in the NN.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # We sample k in [k_min, k_max].\n",
    "    rng = np.random.default_rng(123)\n",
    "    k_vec = rng.uniform(k_min, k_max, N_k)  # random in [k_min, k_max]\n",
    "\n",
    "\n",
    "    # --- Initial condition: u(x,0;k) = sin(pi x) ---\n",
    "    # x in [x_min, x_max], t=0, and all k samples\n",
    "    x_ic = np.linspace(x_min, x_max, N_i)\n",
    "    t_ic = np.array([t_min])   # or [t_min]\n",
    "    x_ic_g, t_ic_g, k_ic_g = np.meshgrid(x_ic, t_ic, k_vec, indexing='ij')\n",
    "\n",
    "    x_u_ic    = x_ic_g.ravel()[:, None]\n",
    "    t_u_ic    = t_ic_g.ravel()[:, None]\n",
    "    k_u_ic    = k_ic_g.ravel()[:, None]\n",
    "    X_u_train_ic = np.hstack([x_u_ic, t_u_ic, k_u_ic])\n",
    "\n",
    "    # --- Boundary conditions: u(0,t;k)=0, u(1,t;k)=0 ---\n",
    "    # We discretise t with 2*N_b points between t_min and t_max.\n",
    "    t_line = np.linspace(t_min, t_max, 2 * N_b)\n",
    "    x_bc_left  = np.array([x_min])\n",
    "    x_bc_right = np.array([x_max])\n",
    "    x_bc = np.concatenate([x_bc_left, x_bc_right], axis=0)  # [0, 1]\n",
    "\n",
    "    x_bc_g, t_bc_g, k_bc_g = np.meshgrid(x_bc, t_line, k_vec, indexing='ij')\n",
    "    x_u_bc    = x_bc_g.ravel()[:, None]\n",
    "    t_u_bc    = t_bc_g.ravel()[:, None]\n",
    "    k_u_bc    = k_bc_g.ravel()[:, None]\n",
    "    X_u_train_bc = np.hstack([x_u_bc, t_u_bc, k_u_bc])\n",
    "\n",
    "    # --- Combine IC and BC into one \"data\" set ---\n",
    "    X_u_train = np.vstack([X_u_train_ic, X_u_train_bc]).astype(np.float32)\n",
    "\n",
    "    # --- Analytic solution for those IC/BC points ---\n",
    "    x_cal    = X_u_train[:, 0]\n",
    "    t_cal    = X_u_train[:, 1]\n",
    "    k_cal    = X_u_train[:, 2]\n",
    "\n",
    "    # Closed-form solution: u(x,t;k) = exp(-k π² t) sin(π x)\n",
    "    u_train = np.exp(-k_cal * (np.pi**2) * t_cal) * np.sin(np.pi * x_cal)\n",
    "    u_train = u_train[:, None].astype(np.float32)\n",
    "\n",
    "    # --- Collocation + validation via LHS in (x, t, k) ---\n",
    "    lb = np.array([x_min, t_min, k_min], dtype=np.float32)\n",
    "    ub = np.array([x_max, t_max, k_max], dtype=np.float32)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    U_all = lhs(3, samples=N_f + N_val)   # Latin Hypercube in [0,1]^3\n",
    "    X_all = lb + (ub - lb) * U_all        # map to [lb, ub] in (x, t, k)\n",
    "    X_f_train = X_all[:N_f]\n",
    "    X_val     = X_all[N_f:]\n",
    "\n",
    "    return X_u_train, u_train, X_f_train, X_val, lb, ub\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Compute global relative L2 error for a trained model at a fixed k\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_rel_L2(model,\n",
    "                   x_min, x_max,\n",
    "                   t_min, t_max,\n",
    "                   k_val=1.0,\n",
    "                   Nx=100, Nt=100,\n",
    "                   device=device):\n",
    "    \"\"\"\n",
    "    Compute global relative L2 error of the model solution against the analytic\n",
    "    solution on a regular (x,t) grid at a fixed physical k = k_val.\n",
    "\n",
    "    rel_L2 = ||u_pred - u_true||_2 / ||u_true||_2\n",
    "    \"\"\"\n",
    "\n",
    "    # Build regular grid in x, t\n",
    "    x_test = np.linspace(x_min, x_max, Nx)\n",
    "    t_test = np.linspace(t_min, t_max, Nt)\n",
    "\n",
    "    T, X = np.meshgrid(t_test, x_test, indexing='ij')  # shape (Nt, Nx)\n",
    "    K    = np.full_like(T, k_val)                   # broadcast\n",
    "\n",
    "    # Flatten to (Nt*Nx, 1) column vectors\n",
    "    x_flat    = X.ravel()[:, None]\n",
    "    t_flat    = T.ravel()[:, None]\n",
    "    k_flat    = K.ravel()[:, None]\n",
    "\n",
    "    # Stack into (Nt*Nx, 3) array and convert to torch tensor\n",
    "    X_star    = np.hstack([x_flat, t_flat, k_flat]).astype(np.float32)\n",
    "    X_star_tf = torch.from_numpy(X_star).to(device)\n",
    "\n",
    "    # NN prediction over the grid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        u_pred = model(X_star_tf).squeeze(1).cpu().numpy().reshape(T.shape)\n",
    "\n",
    "    # Analytic solution on the same grid\n",
    "    u_true = u_true_numpy(X, T, K)\n",
    "\n",
    "    # Global relative L2 error\n",
    "    num = np.linalg.norm(u_pred - u_true)\n",
    "    den = np.linalg.norm(u_true)\n",
    "    rel_L2 = num / den if den > 0 else num\n",
    "\n",
    "    return rel_L2\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Plot training curves (loss, val MSE, max |error|) and save to file\n",
    "# -----------------------------------------------------------------------------\n",
    "def plot_training_curves(loss_values, mse_v_hist, max_errors,\n",
    "                         N_i, N_b, N_k, N_f,\n",
    "                         out_dir=\"sweep_results\"):\n",
    "    \"\"\"\n",
    "    Make the TrainLoss / ValMSE / Max|Error| vs epoch plot and save to file.\n",
    "\n",
    "    Args:\n",
    "        loss_values : list of train loss per epoch\n",
    "        mse_v_hist  : list of (epoch, val_MSE)\n",
    "        max_errors  : list of (epoch, max_abs_error) on validation\n",
    "        N_i, N_b, N_k, N_f : configuration used (for filename)\n",
    "        out_dir     : directory where the PNG is saved\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Training loss epochs\n",
    "    ep_train = range(len(loss_values))\n",
    "\n",
    "    # Validation MSE: unpack (epoch, mse)\n",
    "    ep_val  = [int(i) for i, _ in mse_v_hist]\n",
    "    mse_val = [\n",
    "        (m.detach().cpu().item() if torch.is_tensor(m) else float(m))\n",
    "        for _, m in mse_v_hist\n",
    "    ]\n",
    "\n",
    "    # Max absolute error: unpack (epoch, max_err)\n",
    "    ep_max   = [int(i) for i, _ in max_errors]\n",
    "    max_errs = [\n",
    "        (m.detach().cpu().item() if torch.is_tensor(m) else float(m))\n",
    "        for _, m in max_errors\n",
    "    ]\n",
    "\n",
    "    # Plot curves on log scale (since errors/loss typically span many orders)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(ep_train, loss_values, color='black', label='Train Loss')\n",
    "    plt.plot(ep_val,   mse_val,     color='red',   label='Validation MSE')\n",
    "    plt.plot(ep_max,   max_errs,    color='blue',  label='Max |Error| (Validation)')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss / Error')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Training Loss, Validation MSE, and Max Validation Error vs Iterations')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # File name tagged with the configuration\n",
    "    fname = f\"train_Ni{N_i}_Nb{N_b}_Nk{N_k}_Nf{N_f}.png\"\n",
    "    fpath = os.path.join(out_dir, fname)\n",
    "    plt.savefig(fpath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Run a single experiment for given (N_i, N_b, N_k, N_f)\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_single_experiment(N_i, N_b, N_k, N_f, seed,\n",
    "                          N_val=100,\n",
    "                          Train_epochs=100000,\n",
    "                          learning_rate=5e-4,\n",
    "                          k_val_eval=1.0,\n",
    "                          results_dir=\"sweep_results\"):\n",
    "    \"\"\"\n",
    "    Runs one full experiment:\n",
    "      - builds dataset for given (N_i, N_b, N_k, N_f)\n",
    "      - trains a fresh model\n",
    "      - saves training curve plot\n",
    "      - computes global rel L2 error at k = k_val_eval\n",
    "      - returns a dict with all requested info\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Build dataset (IC+BC data, collocation, validation, bounds)\n",
    "    X_u_train, u_train, X_f_train, X_val, lb, ub = build_dataset(\n",
    "        N_i=N_i, N_b=N_b, N_k=N_k, N_f=N_f, N_val=N_val,\n",
    "        x_min=x_min, x_max=x_max, t_min=t_min, t_max=t_max,\n",
    "        k_min=k_min, k_max=k_max, seed=seed\n",
    "    )\n",
    "\n",
    "    # 2) Initialise a new PINN model for this dataset\n",
    "    model = NeuralNet(layers, lb, ub).to(device).float()\n",
    "\n",
    "    # 3) Train and measure total wall-clock time for this configuration\n",
    "    exp_start = time.time()\n",
    "    loss_values, mse_v_hist, max_errors, best_ep, best_TL, best_v, best_max_err = train(\n",
    "        Train_epochs,\n",
    "        X_u_train,\n",
    "        u_train,\n",
    "        X_f_train,\n",
    "        X_val,\n",
    "        model,\n",
    "        learning_rate\n",
    "    )\n",
    "    total_elapsed = time.time() - exp_start\n",
    "\n",
    "    # 4) Compute global relative L2 error at a chosen k (e.g. k = 1.0)\n",
    "    rel_L2 = compute_rel_L2(\n",
    "        model,\n",
    "        x_min=x_min, x_max=x_max,\n",
    "        t_min=t_min, t_max=t_max,\n",
    "        k_val=k_val_eval,\n",
    "        Nx=100, Nt=100,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 5) Create and save the training curve plot for this run\n",
    "    curve_path = plot_training_curves(\n",
    "        loss_values, mse_v_hist, max_errors,\n",
    "        N_i=N_i, N_b=N_b, N_k=N_k, N_f=N_f,\n",
    "        out_dir=results_dir\n",
    "    )\n",
    "\n",
    "    # 6) Pack all information into a record dictionary\n",
    "    record = {\n",
    "        \"N_i\": N_i,\n",
    "        \"N_b\": N_b,\n",
    "        \"N_k\": N_k,\n",
    "        \"N_f\": N_f,\n",
    "        \"total_elapsed\": total_elapsed,\n",
    "        \"best_ep\": best_ep,\n",
    "        \"best_TL\": best_TL,\n",
    "        \"best_v\": best_v,\n",
    "        \"best_max_err\": best_max_err,\n",
    "        \"rel_L2\": rel_L2,\n",
    "        \"loss_values\": loss_values,\n",
    "        \"mse_v_hist\": mse_v_hist,\n",
    "        \"max_errors\": max_errors,\n",
    "        \"training_curve_path\": curve_path,\n",
    "    }\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Sweep settings and loops for N_f, N_i, N_b, N_k\n",
    "# -----------------------------------------------------------------------------\n",
    "# Network architecture: [ (x, t, k) ] -> hidden layers -> [u]\n",
    "layers = [3, 50, 50, 50, 1]\n",
    "\n",
    "# Domain bounds\n",
    "x_min, x_max = 0.0, 1.0\n",
    "t_min, t_max = 0.0, 0.25\n",
    "k_min, k_max = -5.0, 5.0\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 123\n",
    "    \n",
    "# Base values (same as your current defaults)\n",
    "BASE_N_i = 101\n",
    "BASE_N_b = 51\n",
    "BASE_N_k = 51\n",
    "BASE_N_f = 1000\n",
    "\n",
    "# Training hyperparameters for all sweeps\n",
    "Train_epochs = 100000\n",
    "learning_rate = 0.0005\n",
    "N_val = 100          # number of validation points from LHS\n",
    "k_val_eval = 1.0     # k at which global rel L2 is computed\n",
    "\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Repeat experiment: fixed N_f\n",
    "# ==============================\n",
    "fixed_N_f   = None      # set to an int (e.g. 1000) to override, or None to use BASE_N_f\n",
    "N_repeats   = 5         # how many times to repeat\n",
    "results_rep = []\n",
    "\n",
    "for rep in range(1, N_repeats + 1):\n",
    "    # Decide which N_f to use\n",
    "    N_f_to_use = fixed_N_f if fixed_N_f is not None else BASE_N_f\n",
    "    \n",
    "    print(\n",
    "        f\"\\n=== Repeat {rep}/{N_repeats} with \"\n",
    "        f\"N_f = {N_f_to_use} (N_i={BASE_N_i}, N_b={BASE_N_b}, N_k={BASE_N_k}) ===\"\n",
    "    )\n",
    "\n",
    "    rec = run_single_experiment(\n",
    "        N_i=BASE_N_i,\n",
    "        N_b=BASE_N_b,\n",
    "        N_k=BASE_N_k,\n",
    "        N_f=N_f_to_use,  \n",
    "        seed=123,                 # keep the same seed if you truly want identical setup\n",
    "        # seed=123 + rep          # <- use this instead if you want different randomness per run\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"repeat\",\n",
    "    )\n",
    "    results_rep.append(rec)\n",
    "\n",
    "# Convert list of records to DataFrame for easy inspection and saving\n",
    "df_rep = pd.DataFrame([\n",
    "    {\n",
    "        \"repeat\": i + 1,\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "    }\n",
    "    for i, r in enumerate(results_rep)\n",
    "])\n",
    "\n",
    "print(\"\\nRepeated runs summary:\")\n",
    "display(df_rep)\n",
    "\n",
    "# Save repeated-runs summary table as Excel\n",
    "df_rep.to_excel(\"repeat_summary.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# # ==========================\n",
    "# # Sweep 1: Collocation points N_f\n",
    "# # ==========================\n",
    "# Nf_list = [500, 1000, 2000, 4000]  # values to test for N_f\n",
    "\n",
    "# results_Nf = []\n",
    "\n",
    "# for N_f in Nf_list:\n",
    "#     print(f\"\\n=== Sweep N_f = {N_f} (N_i={BASE_N_i}, N_b={BASE_N_b}, N_k={BASE_N_k}) ===\")\n",
    "#     rec = run_single_experiment(\n",
    "#         N_i=BASE_N_i,\n",
    "#         N_b=BASE_N_b,\n",
    "#         N_k=BASE_N_k,\n",
    "#         N_f=N_f,\n",
    "#         seed=123,\n",
    "#         N_val=N_val,\n",
    "#         Train_epochs=Train_epochs,\n",
    "#         learning_rate=learning_rate,\n",
    "#         k_val_eval=k_val_eval,\n",
    "#         results_dir=\"sweep_Nf\",\n",
    "#     )\n",
    "#     results_Nf.append(rec)\n",
    "\n",
    "# # Convert list of records to DataFrame for easy inspection and saving\n",
    "# df_Nf = pd.DataFrame([\n",
    "#     {\n",
    "#         \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "#         \"total_elapsed\": r[\"total_elapsed\"],\n",
    "#         \"best_ep\": r[\"best_ep\"],\n",
    "#         \"best_TL\": r[\"best_TL\"],\n",
    "#         \"best_v\": r[\"best_v\"],\n",
    "#         \"best_max_err\": r[\"best_max_err\"],\n",
    "#         \"rel_L2\": r[\"rel_L2\"],\n",
    "#         # Histories stored as objects (lists) – still useful inside Python\n",
    "#         # \"loss_values\": r[\"loss_values\"],\n",
    "#         # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "#         # \"max_errors\": r[\"max_errors\"],\n",
    "#         # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "#     }\n",
    "#     for r in results_Nf\n",
    "# ])\n",
    "\n",
    "# print(\"\\nN_f sweep summary:\")\n",
    "# display(df_Nf)\n",
    "\n",
    "# # Save N_f sweep summary table as Excel\n",
    "# df_Nf.to_excel(\"sweep_Nf_summary.xlsx\", index=False)\n",
    "\n",
    "# # ==========================\n",
    "# # Sweep 2: IC points N_i\n",
    "# # ==========================\n",
    "# Ni_list = [21, 51, 101, 201]  # values to test for N_i\n",
    "\n",
    "# results_Ni = []\n",
    "\n",
    "# for N_i in Ni_list:\n",
    "#     print(f\"\\n=== Sweep N_i = {N_i} (N_b={BASE_N_b}, N_k={BASE_N_k}, N_f={BASE_N_f}) ===\")\n",
    "#     rec = run_single_experiment(\n",
    "#         N_i=N_i,\n",
    "#         N_b=BASE_N_b,\n",
    "#         N_k=BASE_N_k,\n",
    "#         N_f=BASE_N_f,\n",
    "#         N_val=N_val,\n",
    "#         Train_epochs=Train_epochs,\n",
    "#         learning_rate=learning_rate,\n",
    "#         k_val_eval=k_val_eval,\n",
    "#         results_dir=\"sweep_Ni\",\n",
    "#         seed=123\n",
    "#     )\n",
    "#     results_Ni.append(rec)\n",
    "\n",
    "# df_Ni = pd.DataFrame([\n",
    "#     {\n",
    "#         \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "#         \"total_elapsed\": r[\"total_elapsed\"],\n",
    "#         \"best_ep\": r[\"best_ep\"],\n",
    "#         \"best_TL\": r[\"best_TL\"],\n",
    "#         \"best_v\": r[\"best_v\"],\n",
    "#         \"best_max_err\": r[\"best_max_err\"],\n",
    "#         \"rel_L2\": r[\"rel_L2\"],\n",
    "#         # \"loss_values\": r[\"loss_values\"],\n",
    "#         # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "#         # \"max_errors\": r[\"max_errors\"],\n",
    "#         # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "#     }\n",
    "#     for r in results_Ni\n",
    "# ])\n",
    "\n",
    "# print(\"\\nN_i sweep summary:\")\n",
    "# display(df_Ni)\n",
    "\n",
    "# df_Ni.to_excel(\"sweep_Ni_summary.xlsx\", index=False)\n",
    "\n",
    "# # ==========================\n",
    "# # Sweep 3: BC points N_b\n",
    "# # ==========================\n",
    "# Nb_list = [11, 21, 51, 101]  # values to test for N_b\n",
    "\n",
    "# results_Nb = []\n",
    "\n",
    "# for N_b in Nb_list:\n",
    "#     print(f\"\\n=== Sweep N_b = {N_b} (N_i={BASE_N_i}, N_k={BASE_N_k}, N_f={BASE_N_f}) ===\")\n",
    "#     rec = run_single_experiment(\n",
    "#         N_i=BASE_N_i,\n",
    "#         N_b=N_b,\n",
    "#         N_k=BASE_N_k,\n",
    "#         N_f=BASE_N_f,\n",
    "#         N_val=N_val,\n",
    "#         Train_epochs=Train_epochs,\n",
    "#         learning_rate=learning_rate,\n",
    "#         k_val_eval=k_val_eval,\n",
    "#         results_dir=\"sweep_Nb\",\n",
    "#         seed=123\n",
    "#     )\n",
    "#     results_Nb.append(rec)\n",
    "\n",
    "# df_Nb = pd.DataFrame([\n",
    "#     {\n",
    "#         \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "#         \"total_elapsed\": r[\"total_elapsed\"],\n",
    "#         \"best_ep\": r[\"best_ep\"],\n",
    "#         \"best_TL\": r[\"best_TL\"],\n",
    "#         \"best_v\": r[\"best_v\"],\n",
    "#         \"best_max_err\": r[\"best_max_err\"],\n",
    "#         \"rel_L2\": r[\"rel_L2\"],\n",
    "#         # \"loss_values\": r[\"loss_values\"],\n",
    "#         # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "#         # \"max_errors\": r[\"max_errors\"],\n",
    "#         # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "#     }\n",
    "#     for r in results_Nb\n",
    "# ])\n",
    "\n",
    "# print(\"\\nN_b sweep summary:\")\n",
    "# display(df_Nb)\n",
    "\n",
    "# df_Nb.to_excel(\"sweep_Nb_summary.xlsx\", index=False)\n",
    "\n",
    "# # ==========================\n",
    "# # Sweep 4: parameter samples N_k\n",
    "# # ==========================\n",
    "# Nk_list = [11, 21, 51, 101]  # values to test for N_k\n",
    "\n",
    "# results_Nk = []\n",
    "\n",
    "# for N_k in Nk_list:\n",
    "#     print(f\"\\n=== Sweep N_k = {N_k} (N_i={BASE_N_i}, N_b={BASE_N_b}, N_f={BASE_N_f}) ===\")\n",
    "#     rec = run_single_experiment(\n",
    "#         N_i=BASE_N_i,\n",
    "#         N_b=BASE_N_b,\n",
    "#         N_k=N_k,\n",
    "#         N_f=BASE_N_f,\n",
    "#         N_val=N_val,\n",
    "#         Train_epochs=Train_epochs,\n",
    "#         learning_rate=learning_rate,\n",
    "#         k_val_eval=k_val_eval,\n",
    "#         results_dir=\"sweep_Nk\",\n",
    "#         seed=123\n",
    "#     )\n",
    "#     results_Nk.append(rec)\n",
    "\n",
    "# df_Nk = pd.DataFrame([\n",
    "#     {\n",
    "#         \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "#         \"total_elapsed\": r[\"total_elapsed\"],\n",
    "#         \"best_ep\": r[\"best_ep\"],\n",
    "#         \"best_TL\": r[\"best_TL\"],\n",
    "#         \"best_v\": r[\"best_v\"],\n",
    "#         \"best_max_err\": r[\"best_max_err\"],\n",
    "#         \"rel_L2\": r[\"rel_L2\"],\n",
    "#         # \"loss_values\": r[\"loss_values\"],\n",
    "#         # \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "#         # \"max_errors\": r[\"max_errors\"],\n",
    "#         # \"training_curve_path\": r[\"training_curve_path\"],\n",
    "#     }\n",
    "#     for r in results_Nk\n",
    "# ])\n",
    "\n",
    "# print(\"\\nN_k sweep summary:\")\n",
    "# display(df_Nk)\n",
    "\n",
    "# df_Nk.to_excel(\"sweep_Nk_summary.xlsx\", index=False)\n",
    "\n",
    "print(\"\\n=== END OF SCRIPT ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758839791644,
     "user": {
      "displayName": "Oscar Zhang",
      "userId": "08884929965206842440"
     },
     "user_tz": -60
    },
    "id": "jQWrqMPo5Apy"
   },
   "outputs": [],
   "source": [
    "# t = data['t'].flatten()[:,None] # read in t and flatten into column vector\n",
    "# x = data['x'].flatten()[:,None] # read in x and flatten into column vector\n",
    "#  # Exact represents the exact solution to the problem, from the data provided\n",
    "# Exact = np.real(data['usol']).T # Exact has structure of nt times nx\n",
    "\n",
    "# print(\"usol shape (nt, nx) = \", Exact.shape)\n",
    "\n",
    "# # We need to find all the x,t coordinate pairs in the domain\n",
    "# X, T = np.meshgrid(x,t)\n",
    "\n",
    "# # Flatten the coordinate grid into pairs of x,t coordinates\n",
    "# X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # coordinates x,t\n",
    "# u_star = Exact.flatten()[:,None]   # corresponding solution value with each coordinate\n",
    "\n",
    "\n",
    "# print(\"X has shape \", X.shape, \", X_star has shape \", X_star.shape, \", u_star has shape \", u_star.shape)\n",
    "\n",
    "# # Domain bounds (-1,1)\n",
    "# lb = X_star.min(axis=0)\n",
    "# ub = X_star.max(axis=0)\n",
    "\n",
    "# print(\"Lower bounds of x,t: \", lb)\n",
    "# print(\"Upper bounds of x,t: \", ub)\n",
    "# print('')\n",
    "# print('The first few entries of X_star are:')\n",
    "# print( X_star[0:5, :] )\n",
    "\n",
    "# print('')\n",
    "# print('The first few entries of u_star are:')\n",
    "# print( u_star[0:5, :] )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1758839791622,
     "user": {
      "displayName": "Oscar Zhang",
      "userId": "08884929965206842440"
     },
     "user_tz": -60
    },
    "id": "fWGYfkhF4JUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? True\n",
      "Device: NVIDIA A2\n",
      "Using device: cuda\n",
      "\n",
      "=== Sweep N_f = 500 (N_i=101, N_b=51, N_k=51) ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_min' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 330\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m N_f \u001b[38;5;129;01min\u001b[39;00m Nf_list:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Sweep N_f = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_f\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (N_i=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_N_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, N_b=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_N_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, N_k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_N_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     rec = \u001b[43mrun_single_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mN_i\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_N_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mN_b\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_N_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mN_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_N_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mN_f\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_f\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mN_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTrain_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTrain_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_val_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_val_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msweep_Nf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m123\u001b[39;49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m     results_Nf.append(rec)\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# Convert list of records to DataFrame for easy inspection and saving\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 247\u001b[39m, in \u001b[36mrun_single_experiment\u001b[39m\u001b[34m(N_i, N_b, N_k, N_f, N_val, Train_epochs, learning_rate, k_val_eval, results_dir, seed)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[33;03mRuns one full experiment:\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[33;03m  - builds dataset for given (N_i, N_b, N_k, N_f)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m \u001b[33;03m  - returns a dict with all requested info\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# 1) Build dataset (IC+BC data, collocation, validation, bounds)\u001b[39;00m\n\u001b[32m    245\u001b[39m X_u_train, u_train, X_f_train, X_val, lb, ub = build_dataset(\n\u001b[32m    246\u001b[39m     N_i=N_i, N_b=N_b, N_k=N_k, N_f=N_f, N_val=N_val,\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     x_min=\u001b[43mx_min\u001b[49m, x_max=x_max, t_min=t_min, t_max=t_max,\n\u001b[32m    248\u001b[39m     k_min=k_min, k_max=k_max, seed=seed\n\u001b[32m    249\u001b[39m )\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# 2) Initialise a new PINN model for this dataset\u001b[39;00m\n\u001b[32m    252\u001b[39m model = NeuralNet(layers, lb, ub).to(device).float()\n",
      "\u001b[31mNameError\u001b[39m: name 'x_min' is not defined"
     ]
    }
   ],
   "source": [
    "# For readability: disable warnings from libraries like matplotlib, etc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "# Make sure torch is imported somewhere above this cell:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "from itertools import product, combinations\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as la\n",
    "from pyDOE import lhs\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogLocator, FuncFormatter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# --- Device Setup ---\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Select the most performant device available (CUDA > MPS > CPU)\n",
    "device = (\n",
    "    torch.device('cuda') if torch.cuda.is_available()\n",
    "    else torch.device('mps') if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else torch.device('cpu')\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Build dataset for arbitrary (N_i, N_b, N_k, N_f)\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_dataset(N_i, N_b, N_k, N_f, N_val,\n",
    "                  x_min=0.0, x_max=1.0,\n",
    "                  t_min=0.0, t_max=0.25,\n",
    "                  k_min=0.2, k_max=2.0,\n",
    "                  seed=123):\n",
    "    \"\"\"\n",
    "    Build training and collocation sets for the parametric heat equation.\n",
    "\n",
    "    Returns:\n",
    "        X_u_train : (N_u, 3) array of data points (x, t, log10(k)) for IC + BC.\n",
    "        u_train   : (N_u, 1) analytic solution at X_u_train.\n",
    "        X_f_train : (N_f, 3) collocation points in (x, t, log10(k)).\n",
    "        X_val     : (N_val, 3) validation points in (x, t, log10(k)).\n",
    "        lb, ub    : lower/upper bounds for normalisation in the NN.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- k and logk ---\n",
    "    # We sample k in [k_min, k_max] but represent it via log10(k).\n",
    "    logk_min = np.log10(k_min)\n",
    "    logk_max = np.log10(k_max)\n",
    "    logk_vec = np.linspace(logk_min, logk_max, N_k)  # equally spaced in log10(k)\n",
    "    k_vec    = 10.0**logk_vec                        # corresponding physical k\n",
    "\n",
    "    # --- Initial condition: u(x,0;k) = sin(pi x) ---\n",
    "    # x in [x_min, x_max], t=0, and all logk samples\n",
    "    x_ic = np.linspace(x_min, x_max, N_i)\n",
    "    t_ic = [0.0]\n",
    "    x_ic_g, t_ic_g, logk_ic_g = np.meshgrid(x_ic, t_ic, logk_vec, indexing='ij')\n",
    "\n",
    "    x_u_ic    = x_ic_g.ravel()[:, None]\n",
    "    t_u_ic    = t_ic_g.ravel()[:, None]\n",
    "    logk_u_ic = logk_ic_g.ravel()[:, None]\n",
    "    X_u_train_ic = np.hstack([x_u_ic, t_u_ic, logk_u_ic])\n",
    "\n",
    "    # --- Boundary conditions: u(0,t;k)=0, u(1,t;k)=0 ---\n",
    "    # We discretise t with 2*N_b points between t_min and t_max.\n",
    "    t_line = np.linspace(t_min, t_max, 2 * N_b)\n",
    "    x_bc_left  = [0.0]\n",
    "    x_bc_right = [1.0]\n",
    "    x_bc = np.concatenate([x_bc_left, x_bc_right], axis=0)  # [0, 1]\n",
    "\n",
    "    x_bc_g, t_bc_g, logk_bc_g = np.meshgrid(x_bc, t_line, logk_vec, indexing='ij')\n",
    "    x_u_bc    = x_bc_g.ravel()[:, None]\n",
    "    t_u_bc    = t_bc_g.ravel()[:, None]\n",
    "    logk_u_bc = logk_bc_g.ravel()[:, None]\n",
    "    X_u_train_bc = np.hstack([x_u_bc, t_u_bc, logk_u_bc])\n",
    "\n",
    "    # --- Combine IC and BC into one \"data\" set ---\n",
    "    X_u_train = np.vstack([X_u_train_ic, X_u_train_bc]).astype(np.float32)\n",
    "\n",
    "    # --- Analytic solution for those IC/BC points ---\n",
    "    x_cal    = X_u_train[:, 0]\n",
    "    t_cal    = X_u_train[:, 1]\n",
    "    logk_cal = X_u_train[:, 2]\n",
    "    k_cal    = 10.0**logk_cal\n",
    "\n",
    "    # Closed-form solution: u(x,t;k) = exp(-k π² t) sin(π x)\n",
    "    u_train = np.exp(-k_cal * (np.pi**2) * t_cal) * np.sin(np.pi * x_cal)\n",
    "    u_train = u_train[:, None].astype(np.float32)\n",
    "\n",
    "    # --- Collocation + validation via LHS in (x, t, logk) ---\n",
    "    lb = np.array([x_min, t_min, logk_min], dtype=np.float32)\n",
    "    ub = np.array([x_max, t_max, logk_max], dtype=np.float32)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    U_all = lhs(3, samples=N_f + N_val)   # Latin Hypercube in [0,1]^3\n",
    "    X_all = lb + (ub - lb) * U_all        # map to [lb, ub] in (x, t, logk)\n",
    "    X_f_train = X_all[:N_f]\n",
    "    X_val     = X_all[N_f:]\n",
    "\n",
    "    return X_u_train, u_train, X_f_train, X_val, lb, ub\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Compute global relative L2 error for a trained model at a fixed k\n",
    "# -----------------------------------------------------------------------------\n",
    "def compute_rel_L2(model,\n",
    "                   k_val=1.0,\n",
    "                   x_min=0.0, x_max=1.0,\n",
    "                   t_min=0.0, t_max=0.25,\n",
    "                   Nx=100, Nt=100,\n",
    "                   device=device):\n",
    "    \"\"\"\n",
    "    Compute global relative L2 error of the model solution against the analytic\n",
    "    solution on a regular (x,t) grid at a fixed physical k = k_val.\n",
    "\n",
    "    rel_L2 = ||u_pred - u_true||_2 / ||u_true||_2\n",
    "    \"\"\"\n",
    "\n",
    "    # Build regular grid in x, t\n",
    "    x_test = np.linspace(x_min, x_max, Nx)\n",
    "    t_test = np.linspace(t_min, t_max, Nt)\n",
    "    logk_val = np.log10(k_val)  # convert to log10(k) for NN input\n",
    "\n",
    "    T, X = np.meshgrid(t_test, x_test, indexing='ij')  # shape (Nt, Nx)\n",
    "    LOGK = np.full_like(T, logk_val)                   # broadcast log10(k_val)\n",
    "\n",
    "    # Flatten to (Nt*Nx, 1) column vectors\n",
    "    x_flat    = X.ravel()[:, None]\n",
    "    t_flat    = T.ravel()[:, None]\n",
    "    logk_flat = LOGK.ravel()[:, None]\n",
    "\n",
    "    # Stack into (Nt*Nx, 3) array and convert to torch tensor\n",
    "    X_star    = np.hstack([x_flat, t_flat, logk_flat]).astype(np.float32)\n",
    "    X_star_tf = torch.from_numpy(X_star).to(device)\n",
    "\n",
    "    # NN prediction over the grid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        u_pred = model(X_star_tf).squeeze(1).cpu().numpy().reshape(T.shape)\n",
    "\n",
    "    # Analytic solution on the same grid\n",
    "    u_true = u_true_numpy(X, T, LOGK)\n",
    "\n",
    "    # Global relative L2 error\n",
    "    num = np.linalg.norm(u_pred - u_true)\n",
    "    den = np.linalg.norm(u_true)\n",
    "    rel_L2 = num / den if den > 0 else num\n",
    "\n",
    "    return rel_L2\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Plot training curves (loss, val MSE, max |error|) and save to file\n",
    "# -----------------------------------------------------------------------------\n",
    "def plot_training_curves(loss_values, mse_v_hist, max_errors,\n",
    "                         N_i, N_b, N_k, N_f,\n",
    "                         out_dir=\"sweep_results\"):\n",
    "    \"\"\"\n",
    "    Make the TrainLoss / ValMSE / Max|Error| vs epoch plot and save to file.\n",
    "\n",
    "    Args:\n",
    "        loss_values : list of train loss per epoch\n",
    "        mse_v_hist  : list of (epoch, val_MSE)\n",
    "        max_errors  : list of (epoch, max_abs_error) on validation\n",
    "        N_i, N_b, N_k, N_f : configuration used (for filename)\n",
    "        out_dir     : directory where the PNG is saved\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Training loss epochs\n",
    "    ep_train = range(len(loss_values))\n",
    "\n",
    "    # Validation MSE: unpack (epoch, mse)\n",
    "    ep_val  = [int(i) for i, _ in mse_v_hist]\n",
    "    mse_val = [\n",
    "        (m.detach().cpu().item() if torch.is_tensor(m) else float(m))\n",
    "        for _, m in mse_v_hist\n",
    "    ]\n",
    "\n",
    "    # Max absolute error: unpack (epoch, max_err)\n",
    "    ep_max   = [int(i) for i, _ in max_errors]\n",
    "    max_errs = [\n",
    "        (m.detach().cpu().item() if torch.is_tensor(m) else float(m))\n",
    "        for _, m in max_errors\n",
    "    ]\n",
    "\n",
    "    # Plot curves on log scale (since errors/loss typically span many orders)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(ep_train, loss_values, color='black', label='Train Loss')\n",
    "    plt.plot(ep_val,   mse_val,     color='red',   label='Validation MSE')\n",
    "    plt.plot(ep_max,   max_errs,    color='blue',  label='Max |Error| (Validation)')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss / Error')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Training Loss, Validation MSE, and Max Validation Error vs Iterations')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # File name tagged with the configuration\n",
    "    fname = f\"train_Ni{N_i}_Nb{N_b}_Nk{N_k}_Nf{N_f}.png\"\n",
    "    fpath = os.path.join(out_dir, fname)\n",
    "    plt.savefig(fpath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Run a single experiment for given (N_i, N_b, N_k, N_f)\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_single_experiment(N_i, N_b, N_k, N_f,\n",
    "                          N_val=100,\n",
    "                          Train_epochs=100000,\n",
    "                          learning_rate=5e-4,\n",
    "                          k_val_eval=1.0,\n",
    "                          results_dir=\"sweep_results\",\n",
    "                          seed=123):\n",
    "    \"\"\"\n",
    "    Runs one full experiment:\n",
    "      - builds dataset for given (N_i, N_b, N_k, N_f)\n",
    "      - trains a fresh model\n",
    "      - saves training curve plot\n",
    "      - computes global rel L2 error at k = k_val_eval\n",
    "      - returns a dict with all requested info\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Build dataset (IC+BC data, collocation, validation, bounds)\n",
    "    X_u_train, u_train, X_f_train, X_val, lb, ub = build_dataset(\n",
    "        N_i=N_i, N_b=N_b, N_k=N_k, N_f=N_f, N_val=N_val,\n",
    "        x_min=x_min, x_max=x_max, t_min=t_min, t_max=t_max,\n",
    "        k_min=k_min, k_max=k_max, seed=seed\n",
    "    )\n",
    "\n",
    "    # 2) Initialise a new PINN model for this dataset\n",
    "    model = NeuralNet(layers, lb, ub).to(device).float()\n",
    "\n",
    "    # 3) Train and measure total wall-clock time for this configuration\n",
    "    exp_start = time.time()\n",
    "    loss_values, mse_v_hist, max_errors, best_ep, best_TL, best_v, best_max_err = train(\n",
    "        Train_epochs,\n",
    "        X_u_train,\n",
    "        u_train,\n",
    "        X_f_train,\n",
    "        X_val,\n",
    "        model,\n",
    "        learning_rate\n",
    "    )\n",
    "    total_elapsed = time.time() - exp_start\n",
    "\n",
    "    # 4) Compute global relative L2 error at a chosen k (e.g. k = 1.0)\n",
    "    rel_L2 = compute_rel_L2(\n",
    "        model,\n",
    "        k_val=k_val_eval,\n",
    "        x_min=x_min, x_max=x_max,\n",
    "        t_min=t_min, t_max=t_max,\n",
    "        Nx=100, Nt=100,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 5) Create and save the training curve plot for this run\n",
    "    curve_path = plot_training_curves(\n",
    "        loss_values, mse_v_hist, max_errors,\n",
    "        N_i=N_i, N_b=N_b, N_k=N_k, N_f=N_f,\n",
    "        out_dir=results_dir\n",
    "    )\n",
    "\n",
    "    # 6) Pack all information into a record dictionary\n",
    "    record = {\n",
    "        \"N_i\": N_i,\n",
    "        \"N_b\": N_b,\n",
    "        \"N_k\": N_k,\n",
    "        \"N_f\": N_f,\n",
    "        \"total_elapsed\": total_elapsed,\n",
    "        \"best_ep\": best_ep,\n",
    "        \"best_TL\": best_TL,\n",
    "        \"best_v\": best_v,\n",
    "        \"best_max_err\": best_max_err,\n",
    "        \"rel_L2\": rel_L2,\n",
    "        \"loss_values\": loss_values,\n",
    "        \"mse_v_hist\": mse_v_hist,\n",
    "        \"max_errors\": max_errors,\n",
    "        \"training_curve_path\": curve_path,\n",
    "    }\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Sweep settings and loops for N_f, N_i, N_b, N_k\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Base values (same as your current defaults)\n",
    "BASE_N_i = 101\n",
    "BASE_N_b = 51\n",
    "BASE_N_k = 51\n",
    "BASE_N_f = 1000\n",
    "\n",
    "# Training hyperparameters for all sweeps\n",
    "Train_epochs = 100000\n",
    "learning_rate = 0.0005\n",
    "N_val = 100          # number of validation points from LHS\n",
    "k_val_eval = 1.0     # k at which global rel L2 is computed\n",
    "\n",
    "# ==========================\n",
    "# Sweep 1: Collocation points N_f\n",
    "# ==========================\n",
    "Nf_list = [500, 1000, 2000, 4000]  # values to test for N_f\n",
    "\n",
    "results_Nf = []\n",
    "\n",
    "for N_f in Nf_list:\n",
    "    print(f\"\\n=== Sweep N_f = {N_f} (N_i={BASE_N_i}, N_b={BASE_N_b}, N_k={BASE_N_k}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=BASE_N_i,\n",
    "        N_b=BASE_N_b,\n",
    "        N_k=BASE_N_k,\n",
    "        N_f=N_f,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Nf\",\n",
    "        seed=123\n",
    "    )\n",
    "    results_Nf.append(rec)\n",
    "\n",
    "# Convert list of records to DataFrame for easy inspection and saving\n",
    "df_Nf = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        # Histories stored as objects (lists) – still useful inside Python\n",
    "        \"loss_values\": r[\"loss_values\"],\n",
    "        \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        \"max_errors\": r[\"max_errors\"],\n",
    "        \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Nf\n",
    "])\n",
    "\n",
    "print(\"\\nN_f sweep summary:\")\n",
    "display(df_Nf)\n",
    "\n",
    "# Save N_f sweep summary table as CSV\n",
    "df_Nf.to_csv(\"sweep_Nf_summary.csv\", index=False)\n",
    "\n",
    "# ==========================\n",
    "# Sweep 2: IC points N_i\n",
    "# ==========================\n",
    "Ni_list = [21, 51, 101, 201]  # values to test for N_i\n",
    "\n",
    "results_Ni = []\n",
    "\n",
    "for N_i in Ni_list:\n",
    "    print(f\"\\n=== Sweep N_i = {N_i} (N_b={BASE_N_b}, N_k={BASE_N_k}, N_f={BASE_N_f}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=N_i,\n",
    "        N_b=BASE_N_b,\n",
    "        N_k=BASE_N_k,\n",
    "        N_f=BASE_N_f,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Ni\",\n",
    "        seed=123\n",
    "    )\n",
    "    results_Ni.append(rec)\n",
    "\n",
    "df_Ni = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        \"loss_values\": r[\"loss_values\"],\n",
    "        \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        \"max_errors\": r[\"max_errors\"],\n",
    "        \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Ni\n",
    "])\n",
    "\n",
    "print(\"\\nN_i sweep summary:\")\n",
    "display(df_Ni)\n",
    "\n",
    "df_Ni.to_csv(\"sweep_Ni_summary.csv\", index=False)\n",
    "\n",
    "# ==========================\n",
    "# Sweep 3: BC points N_b\n",
    "# ==========================\n",
    "Nb_list = [11, 21, 51, 101]  # values to test for N_b\n",
    "\n",
    "results_Nb = []\n",
    "\n",
    "for N_b in Nb_list:\n",
    "    print(f\"\\n=== Sweep N_b = {N_b} (N_i={BASE_N_i}, N_k={BASE_N_k}, N_f={BASE_N_f}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=BASE_N_i,\n",
    "        N_b=N_b,\n",
    "        N_k=BASE_N_k,\n",
    "        N_f=BASE_N_f,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Nb\",\n",
    "        seed=123\n",
    "    )\n",
    "    results_Nb.append(rec)\n",
    "\n",
    "df_Nb = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        \"loss_values\": r[\"loss_values\"],\n",
    "        \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        \"max_errors\": r[\"max_errors\"],\n",
    "        \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Nb\n",
    "])\n",
    "\n",
    "print(\"\\nN_b sweep summary:\")\n",
    "display(df_Nb)\n",
    "\n",
    "df_Nb.to_csv(\"sweep_Nb_summary.csv\", index=False)\n",
    "\n",
    "# ==========================\n",
    "# Sweep 4: parameter samples N_k\n",
    "# ==========================\n",
    "Nk_list = [11, 21, 51, 101]  # values to test for N_k\n",
    "\n",
    "results_Nk = []\n",
    "\n",
    "for N_k in Nk_list:\n",
    "    print(f\"\\n=== Sweep N_k = {N_k} (N_i={BASE_N_i}, N_b={BASE_N_b}, N_f={BASE_N_f}) ===\")\n",
    "    rec = run_single_experiment(\n",
    "        N_i=BASE_N_i,\n",
    "        N_b=BASE_N_b,\n",
    "        N_k=N_k,\n",
    "        N_f=BASE_N_f,\n",
    "        N_val=N_val,\n",
    "        Train_epochs=Train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        k_val_eval=k_val_eval,\n",
    "        results_dir=\"sweep_Nk\",\n",
    "        seed=123\n",
    "    )\n",
    "    results_Nk.append(rec)\n",
    "\n",
    "df_Nk = pd.DataFrame([\n",
    "    {\n",
    "        \"N_i\": r[\"N_i\"], \"N_b\": r[\"N_b\"], \"N_k\": r[\"N_k\"], \"N_f\": r[\"N_f\"],\n",
    "        \"total_elapsed\": r[\"total_elapsed\"],\n",
    "        \"best_ep\": r[\"best_ep\"],\n",
    "        \"best_TL\": r[\"best_TL\"],\n",
    "        \"best_v\": r[\"best_v\"],\n",
    "        \"best_max_err\": r[\"best_max_err\"],\n",
    "        \"rel_L2\": r[\"rel_L2\"],\n",
    "        \"loss_values\": r[\"loss_values\"],\n",
    "        \"mse_v_hist\": r[\"mse_v_hist\"],\n",
    "        \"max_errors\": r[\"max_errors\"],\n",
    "        \"training_curve_path\": r[\"training_curve_path\"],\n",
    "    }\n",
    "    for r in results_Nk\n",
    "])\n",
    "\n",
    "print(\"\\nN_k sweep summary:\")\n",
    "display(df_Nk)\n",
    "\n",
    "df_Nk.to_csv(\"sweep_Nk_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758839791644,
     "user": {
      "displayName": "Oscar Zhang",
      "userId": "08884929965206842440"
     },
     "user_tz": -60
    },
    "id": "jQWrqMPo5Apy"
   },
   "outputs": [],
   "source": [
    "# t = data['t'].flatten()[:,None] # read in t and flatten into column vector\n",
    "# x = data['x'].flatten()[:,None] # read in x and flatten into column vector\n",
    "#  # Exact represents the exact solution to the problem, from the data provided\n",
    "# Exact = np.real(data['usol']).T # Exact has structure of nt times nx\n",
    "\n",
    "# print(\"usol shape (nt, nx) = \", Exact.shape)\n",
    "\n",
    "# # We need to find all the x,t coordinate pairs in the domain\n",
    "# X, T = np.meshgrid(x,t)\n",
    "\n",
    "# # Flatten the coordinate grid into pairs of x,t coordinates\n",
    "# X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # coordinates x,t\n",
    "# u_star = Exact.flatten()[:,None]   # corresponding solution value with each coordinate\n",
    "\n",
    "\n",
    "# print(\"X has shape \", X.shape, \", X_star has shape \", X_star.shape, \", u_star has shape \", u_star.shape)\n",
    "\n",
    "# # Domain bounds (-1,1)\n",
    "# lb = X_star.min(axis=0)\n",
    "# ub = X_star.max(axis=0)\n",
    "\n",
    "# print(\"Lower bounds of x,t: \", lb)\n",
    "# print(\"Upper bounds of x,t: \", ub)\n",
    "# print('')\n",
    "# print('The first few entries of X_star are:')\n",
    "# print( X_star[0:5, :] )\n",
    "\n",
    "# print('')\n",
    "# print('The first few entries of u_star are:')\n",
    "# print( u_star[0:5, :] )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
